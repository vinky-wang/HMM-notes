# Major Earthquake Analysis {#eq}

We apply the concepts in the earlier chapters to the series of annual counts of major earthquakes (i.e. magnitude 7 and above) dataset from the textbook. 

The data can be read into **R** 

```{r}
eq_dat <- read.table("http://www.hmms-for-time-series.de/second/data/earthquakes.txt")
```


```{r echo=F, message=F, warning=F}
# Load packages
library(kableExtra)
library(tidyverse)
library(gridExtra)
```


```{r e1, fig.cap="Number of major earthquakes worldwide from 1900-2006.", echo=F}
plot(eq_dat, type = "b", pch=19, cex=0.5, xlab="", ylab="Count", main="")
```

```{r e2, fig.cap="Histogram of major earthquakes overlaid with a Poisson density of mean equal to the observed counts.", echo=F}
{hist(eq_dat$V2, freq=FALSE, breaks=100, xlab = "Count", main="")
points(0:max(eq_dat$V2), dpois(0:max(eq_dat$V2), mean(eq_dat$V2)),
      col="hotpink", pch=16)
}

```

```{r e4, echo=F}
eq_dat %>%
  summarise(mean = round(mean(V2), 2),
            variance = round((sd(V2))^2, 2)) %>%
  kbl(caption="Summary of the major earthquakes.") %>%
  kable_minimal(full_width=T)

```


### Fitting a Poisson Mixture Distribution

**Note:** See [Preliminaries](#prelim)

We may consider fitting a Poisson mixture distribution since earthquake counts are unbounded and there appears to be overdispersion as evident by the multimodal peaks in Figure \@ref(fig:e2) and sample variance $s^2 \approx 52 > 19 \approx \bar{x}$ in Table \@ref(tab:e4). 

Suppose $m=3$ and the three components are Poisson-distributed with means $\lambda_1, \lambda_2$, and $\lambda_3$. Let $\delta_1$, $\delta_2$, and $\delta_3$ be the respective mixing parameters. The mixture distribution $p$ is given by

$$p(x) = \delta_1 \frac{\lambda_1^x e^{-\lambda_1}}{x!} + \delta_2 \frac{\lambda_2^x e^{-\lambda_2}}{x!} + \delta_3 \frac{\lambda_3^x e^{-\lambda_3}}{x!}$$

The likelihood is given by

$$L(\lambda_1, \lambda_2, \lambda_3, \delta_1, \delta_2|x_1, \dots, x_n) = \prod_{i=1}^n \left( \delta_1 \frac{\lambda_1^{x_i} e^{-\lambda_1}}{x_i!} + \delta_2 \frac{\lambda_2^{x_i} e^{-\lambda_2}}{x_i!} + (1 - \delta_1 - \delta_2) \frac{\lambda_3^{x_i} e^{-\lambda_3}}{x_i!} \right)$$

The log-likelihood can be maximized using an unconstrained optimizer `nlm` (in **R**) on reparameterized parameters by the following:

**Step 1**:  Reparameterize the "natural parameters" $\boldsymbol{\delta}$ and $\boldsymbol{\lambda}$ into "working parameters" $\eta_i = \log \lambda_i \qquad{(i = 1, \dots, m)}$ and $\tau_i = \log \left(\frac{\delta_i}{1 - \sum_{j=2}^m \delta_j}\right) (i = 2, \dots, m)$ 

```{r}
n2w <- function(lambda, delta)log(c(lambda, delta[-1]/(1-sum(delta[-1]))))
```

**Step 2**: Compute the negative log-likelihood using the working parameters

```{r}
mllk <- function(wpar, x){
  zzz <- w2n(wpar)
  -sum(log(outer(x, zzz$lambda, dpois)%*%zzz$delta))}
```

**Step 3**: Transform the parameters back into natural parameters

```{r}
w2n <- function(wpar){
  m <- (length(wpar)+1)/2
  lambda <- exp(wpar[1:m])
  delta <- exp(c(0, wpar[(m+1):(2*m-1)]))
  return(list(lambda=lambda, delta=delta/sum(delta)))}
```

Hence, for a Poisson mixture distribution with means $\lambda_1, = 10, \lambda_2 = 20, \lambda_3 = 25$ and probabilities $\delta_1 = \delta_2 = \delta_3 = \frac{1}{3}$, 

```{r, warning=F, message=F}
x <- eq_dat$V2
wpar <- n2w(c(10, 20, 25), c(1,1,1)/3)

```

```{r eval=F}
w2n(nlm(mllk, wpar, x)$estimate)
```

we obtain the following parameter estimates

```{r echo=F, warning=F, message=F}
data.frame(w2n(nlm(mllk, wpar, x)$estimate)) %>%
    kbl(caption="Parameter estimates for the fitted three component Poisson independent mixture model.") %>%
  kable_minimal(full_width=T)
```

and the negative log likelihood 

```{r, warning=F, message=F}
nlm(mllk, wpar, x)$minimum
```




**Note:** See Table 1.2 and Figure 1.4 of the textbook for a comparison of fitted mixture models with varying number of components. 


### Fitting a Poisson-HMM by Numerical Maximization

**Note:** See [Introduction to Hidden Markov Models](#introhmm) and Appendix A of the textbook. 


```{r acf, fig.cap="Autocorrelation plot for major earthquakes.", echo=F}
v <- acf(eq_dat$V2, plot=FALSE)
plot(v, main="")
```

Instead of fitting an (independent) Poisson mixture distribution, we may consider fitting a Poisson-HMM to account for the strong, positive serial dependence as evident in Figure \@ref(fig:acf). 

**Step 1:** Reparameterize the "natural parameters" $\boldsymbol{\Gamma}, \boldsymbol{\lambda}$, and $\boldsymbol{\delta}$ to the "working parameters".

Set 
$$\eta_i = \log \lambda_i \qquad{(i = 1, \dots, m)}$$ 

$$\nu_{ij} = \begin{cases} 1 & \qquad{\text{for } i = j} \\ \log(\gamma_{ij}) & \qquad{\text{for } i \neq j} \end{cases}$$

$$\delta_i = \begin{cases} \log \left(\frac{(\delta_2, \dots, \delta_m)}{\delta_1}\right) & \qquad{\text{if not stationary}}\\ \text{NULL} & \qquad{\text{otherwise}} \end{cases}$$

See [Reparameterization to Avoid Constraints](#reparam). 

```{r}
pois.HMM.pn2pw <- function(m,lambda,gamma,delta=NULL,stationary=TRUE){ 
  
  # eta = log(lambda)
  tlambda <- log(lambda)
  if(m==1) return(tlambda)
 
  # For i = j, nu_ii = 1 
  foo     <- log(gamma/diag(gamma))
  
  # For i != j, nu_ij = log(gamma_ij)
  tgamma  <- as.vector(foo[!diag(m)])
  
  # If stationary, set to pi = null
  if(stationary) {tdelta  <- NULL}
  # Otherwise, pi = log((delta_2, ..., delta_m)/delta_1)
    else {tdelta <- log(delta[-1]/delta[1])}
 
  parvect <- c(tlambda,tgamma,tdelta)
  return(parvect)
}
```

**Note:** `pois.HMM.pn2pw` takes in arguments `m` number of states (numeric), `lambda` Poisson means (vector), `delta` initial probabilities, and `stationary` (`TRUE/FALSE`). By default, the MC is assumed to be stationary with the initial distribution equal to the stationary distribution. The function returns a vector containing the working parameters. 


**Step 2:** Compute the log-likelihood by the forward algorithm using the scaling strategy. 

The general algorithm is:

$w_1 \leftarrow \boldsymbol{\delta P} (x_1) \boldsymbol{1'};$ $\boldsymbol{\phi}_1 \leftarrow \boldsymbol{\delta P} (x_1);$ $l \leftarrow \log w_1$

for $t=2, 3, \dots, T$

\begin{align}
\boldsymbol{v} 
&\leftarrow \boldsymbol{\phi}_{t-1} \boldsymbol{\Gamma P} (x_t)\\
u & \leftarrow \boldsymbol{v 1'}\\
l &\leftarrow l + \log u\\
\boldsymbol{\phi}_t & \leftarrow \frac{\boldsymbol{v}}{u}
\end{align}


See [Scaling the Likelihood Computation](#likscale) and Note (4).

```{r}
pois.HMM.lforward <- function(x,mod){
  
  n             <- length(x)
  lalpha        <- matrix(NA,mod$m,n)
  
  # At time t=1
  foo           <- mod$delta*dpois(x[1],mod$lambda)
  sumfoo        <- sum(foo)
  lscale        <- log(sumfoo)
  foo           <- foo/sumfoo
  
  lalpha[,1]    <- lscale+log(foo)
  
  # For t > 1
  for (i in 2:n){
    foo          <- foo%*%mod$gamma*dpois(x[i],mod$lambda)
    sumfoo       <- sum(foo)
    lscale       <- lscale+log(sumfoo)
    foo          <- foo/sumfoo
    lalpha[,i]   <- log(foo)+lscale
    }
  return(lalpha)
}
```

**Note:** `pois.HMM.lforward` takes in arguments `x` data and `mod` list consisting of `m` states, `lambda` means for the Poisson state-dependent distribution, and `delta` initial probabilities. 

**Step 3:** Maximize the log-likelihood/Minimize the negative log-likelihood

The function `nlm` in **R** performs the minimization of any function using a Newton-type algorithm. In order to minimize the negative log-likelihood, we first write a function that compute the negative log-likelihood from the working parameters. The same forward algorithm scheme applies. 

```{r}
pois.HMM.mllk <- function(parvect,x,m,stationary=TRUE,...)
{
 if(m==1) return(-sum(dpois(x,exp(parvect),log=TRUE)))
 n        <- length(x)
 pn       <- pois.HMM.pw2pn(m,parvect,stationary=stationary)
 foo      <- pn$delta*dpois(x[1],pn$lambda)
 sumfoo   <- sum(foo)
 lscale   <- log(sumfoo)
 foo      <- foo/sumfoo
 for (i in 2:n)
   {
   if(!is.na(x[i])){P<-dpois(x[i],pn$lambda)}
     else {P<-rep(1,m)}
   foo    <- foo %*% pn$gamma*P
   sumfoo <- sum(foo)
   lscale <- lscale+log(sumfoo)
   foo    <- foo/sumfoo
   }
 mllk     <- -lscale
 return(mllk)
}

```


**Step 4**: Transform the parameters back into natural parameters. 

Set 

$$\hat{\lambda_i} = \exp(\hat{\eta_i}) \qquad{(i = 1, \dots, m)}$$

$$\hat{\gamma}_{ij} = \frac{\exp(\hat{\nu_{ik}})}{\sum_{k=1, k \neq i}^m\exp(\hat{\nu_ik})} \qquad{\text{for } (i, j = 1, \dots, m)}$$

$$\hat{\boldsymbol{\delta}} = \begin{cases} \boldsymbol{1} (\boldsymbol{I} - \hat{\boldsymbol{\Gamma}} + \boldsymbol{U})^{-1}& \qquad{\text{if stationary}}\\ \frac{(1, \exp(\hat{\delta_2}), \dots, \exp(\hat{\delta}_m))}{1 + \exp(\hat{\delta_2}) + \dots + \exp(\hat{\delta}_m)} & \qquad{\text{otherwise}} \end{cases}$$

```{r}
pois.HMM.pw2pn <- function(m,parvect,stationary=TRUE){
 lambda        <- exp(parvect[1:m])
 gamma         <- diag(m)
 if (m==1) return(list(lambda=lambda,gamma=gamma,delta=1))
 gamma[!gamma] <- exp(parvect[(m+1):(m*m)])
 gamma         <- gamma/apply(gamma,1,sum)
 if(stationary){delta<-solve(t(diag(m)-gamma+1),rep(1,m))}
   else {foo<-c(1,exp(parvect[(m*m+1):(m*m+m-1)]))
   delta<-foo/sum(foo)}
 return(list(lambda=lambda,gamma=gamma,delta=delta))
}
```

**Note:** `pois.HMM.pw2pn` takes in arguments `m` number of states, `parvect` working parameters (vector), and `stationary` (`TRUE/FALSE`). By default, the MC is assumed to be stationary with the initial distribution equal to the stationary distribution. The function returns a list containing the natural parameters. 


Then putting everything together 

```{r}
pois.HMM.mle <- function(x,m,lambda0,gamma0,delta0=NULL,stationary=TRUE,...){
 parvect0  <- pois.HMM.pn2pw(m,lambda0,gamma0,delta0,stationary=stationary)
 mod       <- nlm(pois.HMM.mllk,parvect0,x=x,m=m,stationary=stationary)
 pn        <- pois.HMM.pw2pn(m=m,mod$estimate,stationary=stationary)
 mllk      <- mod$minimum
 np        <- length(parvect0)
 AIC       <- 2*(mllk+np)
 n         <- sum(!is.na(x))
 BIC       <- 2*mllk+np*log(n)
 list(m=m,lambda=pn$lambda,gamma=pn$gamma,delta=pn$delta,code=mod$code,mllk=mllk,AIC=AIC,BIC=BIC)
}
```

Hence, for a three-state Poisson-HMM with parameters $\boldsymbol{\lambda} = (10, 20, 25)$, $\boldsymbol{\Gamma} = \begin{pmatrix} 0.8 & 0.1 & 0.1\\ 0.1 & 0.8 & 0.1\\ 0.1 & 0.1 & 0.8 \end{pmatrix}$,

if we assume that the MC is stationary 

```{r warning=F, message=F}
eq_init <- list(m=3, 
            lambda=c(10, 20, 25), 
            gamma = matrix(c(0.8,0.1,0.1,0.1,0.8,0.1,0.1,0.1,0.8),3,3,byrow=TRUE),
            delta=c(1, 1, 1)/3
            )

(mod3s<-pois.HMM.mle(x,eq_init$m,eq_init$lambda,eq_init$gamma,stationary=TRUE))
```

if we do not assume stationary and that $\boldsymbol{\delta} = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$ 

```{r, warning=F, message=F}
(mod3h <- pois.HMM.mle(x,eq_init$m,eq_init$lambda,eq_init$gamma,delta=c(1,1,1)/3,stationary=FALSE))
```

We can also perform parameteric bootstrapping to obtain confidence intervals. 

**Note:** See [Obtaining Standard Errors and Confidence Intervals](#boot). 

**Step 1**: Fit the model

```{r warning=F, message=F}
eq_mle <- pois.HMM.mle(x,eq_init$m,eq_init$lambda,eq_init$gamma,delta=c(1,1,1)/3,stationary=FALSE)
```

**Step 2**: Generate a bootstrap sample from the fitted model.

The following function generates a sample from a Poisson-HMM of length `ns` from starting values `m` number of states, `lambda` means for the Poisson state-dependent distribution, and `delta` initial probabilities which are stored as `mod` (list).  

```{r}
pois.HMM.generate_sample  <- function(ns,mod){
 mvect                    <- 1:mod$m
 state                    <- numeric(ns)
 state[1]                 <- sample(mvect,1,prob=mod$delta)
 for (i in 2:ns) state[i] <- sample(mvect,1,prob=mod$gamma[state[i-1],])
 x                        <- rpois(ns,lambda=mod$lambda[state])
 return(x)
}
```

```{r}
boot_sample <- pois.HMM.generate_sample(length(x), eq_mle)
```



**Step 3**: Estimate parameters using the bootstrap sample.

```{r}
boot_mle <- pois.HMM.mle(boot_sample, eq_mle$m, eq_mle$lambda, eq_mle$gamma)
```

**Step 4**: Repeat (for a large) B times. 

Then putting everything together

```{r}
pois.HMM.boot <- function(B, x, m, lambda0, gamma0, delta0=NULL, stationary=TRUE){
  
  # Fit the model
  mod <- pois.HMM.mle(x, m, lambda0, gamma0, stationary=stationary)
  
  # Initialize
  bs_sample <- list()
  bs_mod <- list()
  
  # Generate B bootstrap samples
  for(i in 1:B){
    a = TRUE
    while(a){
      tryCatch({
        set.seed(i)
        bs_sample[[i]] <- pois.HMM.generate_sample(length(x), mod)
        bs_mod[[i]] <- pois.HMM.mle(bs_sample[[i]], m, mod$lambda, mod$gamma, mod$delta)
        a = FALSE
        },
        error=function(w){
          a = TRUE
        })
    }
    }
  return(bs_mod)
}

```

**Note:** Some errors would occasionally arise from solving the stationary distribution of bootstrap samples that may not have had a unique stationary distribution. To overcome this, the `tryCatch` function was used to throw out these problematic samples and regenerate a new sample. As there were at most two samples that resulted in the error, the generated bootstrap samples should still be representative of the dataset. 

Hence, 

```{r warning=F, message=F}
eq_boot <- pois.HMM.boot(500, x, eq_init$m, eq_init$lambda, eq_init$gamma)
```

and the $90\%$ confidence interval using the "percentile method"

```{r}
# Create dataframe for parameters from bootstrap samples
eq_lambda_boot <- sapply(eq_boot, "[[", 2)
eq_lambda_boot <- t(eq_lambda_boot)
eq_lambda_boot <- as.data.frame(eq_lambda_boot)

eq_gamma_boot <- sapply(eq_boot, "[[", 3)
eq_gamma_boot <- t(eq_gamma_boot)
eq_gamma_boot <- as.data.frame(eq_gamma_boot)

eq_delta_boot <- sapply(eq_boot, "[[", 4)
eq_delta_boot <- t(eq_delta_boot)
eq_delta_boot <- as.data.frame(eq_delta_boot)

# Obtain the 90% confidence interval
eq_boot_df <- t(cbind(sapply(eq_lambda_boot, quantile, probs=c(0.05, 0.95)),
                           sapply(eq_gamma_boot, quantile, probs=c(0.05, 0.95)),
                           sapply(eq_delta_boot, quantile, probs=c(0.05, 0.95)))) %>% 
  as.data.frame() 

rownames(eq_boot_df)[1:3] <- paste0("lambda", 1:3)
rownames(eq_boot_df)[4:6] <- paste0("gamma", 1, 1:3)
rownames(eq_boot_df)[7:9] <- paste0("gamma", 2, 1:3)
rownames(eq_boot_df)[10:12] <- paste0("gamma", 3, 1:3)
rownames(eq_boot_df)[13:15] <- paste0("delta", 1:3)


eq_boot_df %>%
  round(4) %>%
  kbl(caption="Bootstrap 90% Confidence Intervals for the parameters of the three-state HMM.") %>%
  kable_minimal(full_width=T)

```

```{r boot, fig.cap="500 Bootstrap samples for the state-dependent parameters of the fitted three-state Poisson-HMM. The area within the red dotted lines correspond to the 90% confidence interval.", echo=F}

l1_hist <- ggplot(eq_lambda_boot, aes(x=V1)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[1], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[1], color="red", linetype="dashed") +
  labs(x=expression(lambda[1]))

l2_hist <- ggplot(eq_lambda_boot, aes(x=V2)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[2], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[2], color="red", linetype="dashed") +
  labs(x=expression(lambda[2]))

l3_hist <- ggplot(eq_lambda_boot, aes(x=V3)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[3], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[3], color="red", linetype="dashed") +
  labs(x=expression(lambda[3]))

grid.arrange(l1_hist, l2_hist, l3_hist)
```

```{r boot3, fig.cap="500 Bootstrap samples for the initial probabilities of the fitted three-state Poisson-HMM. The area within the red dotted lines correspond to the 90% confidence interval.", echo=F}

d1 <- ggplot(eq_delta_boot, aes(x=V1)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[13], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[13], color="red", linetype="dashed") +
  labs(x=expression(delta[1]))

d2 <- ggplot(eq_delta_boot, aes(x=V2)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[14], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[14], color="red", linetype="dashed") +
  labs(x=expression(delta[2]))

d3 <- ggplot(eq_delta_boot, aes(x=V3)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[15], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[15], color="red", linetype="dashed") +
  labs(x=expression(delta[3]))

grid.arrange(d1, d2, d3, ncol=3)

```

```{r boot2, fig.cap="500 Bootstrap samples for the transition probabilities of the fitted three-state Poisson-HMM. The area within the red dotted lines correspond to the 90% confidence interval.", echo=F}
g1 <- ggplot(eq_gamma_boot, aes(x=V1)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[4], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[4], color="red", linetype="dashed") +
  xlab(expression(gamma[11]))

g2 <- ggplot(eq_gamma_boot, aes(x=V2)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[5], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[5], color="red", linetype="dashed") +
  xlab(expression(gamma[12]))

g3 <- ggplot(eq_gamma_boot, aes(x=V3)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[6], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[6], color="red", linetype="dashed") +
  xlab(expression(gamma[13])) 

g4 <- ggplot(eq_gamma_boot, aes(x=V4)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[7], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[7], color="red", linetype="dashed") +
  xlab(expression(gamma[21]))

g5 <- ggplot(eq_gamma_boot, aes(x=V5)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[8], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[8], color="red", linetype="dashed") +
  xlab(expression(gamma[22]))

g6 <- ggplot(eq_gamma_boot, aes(x=V6)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[9], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[9], color="red", linetype="dashed") +
  xlab(expression(gamma[23]))

g7 <- ggplot(eq_gamma_boot, aes(x=V7)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[10], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[10], color="red", linetype="dashed") +
  xlab(expression(gamma[31]))

g8 <- ggplot(eq_gamma_boot, aes(x=V8)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[11], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[11], color="red", linetype="dashed") +
  xlab(expression(gamma[32]))

g9 <- ggplot(eq_gamma_boot, aes(x=V9)) +
  geom_histogram(bins=100, color="black", fill="grey") +
  theme_minimal() + 
  geom_vline(xintercept=eq_boot_df$`5%`[12], color="red", linetype="dashed") + 
  geom_vline(xintercept=eq_boot_df$`95%`[12], color="red", linetype="dashed") +
  xlab(expression(gamma[33]))


grid.arrange(g1, g2, g3, g4, g5, g6, g7, g8, g9,ncol=3)
```

We can also fit the three-state Poisson-HMM from the above using the `momentuHMM` package.

```{r warning=F, message=F}
library(momentuHMM)
colnames(eq_dat) <- c("year", "count")
```

The data must be converted into the `prepData` object to use `fitHMM`. 

```{r warning=F, message=F}
# Convert dataframe into prepData object
eq_prepped <- prepData(eq_dat,
                       coordNames = NULL,
                       covNames =  c("count"))

# Name the states
stateNames <- c("state1", "state2", "state3")

# Fit intercept only model
formula = ~ 1

eq_fit1 <- fitHMM(eq_prepped, 
               nbStates=3, 
               dist=list(count="pois"),
               Par0 = list(count = eq_init$lambda, delta=rep(1, 3)/3),
               formula = formula,
               stateNames = stateNames,
               stationary = FALSE)
eq_fit1
```

The resulting MLEs are very similar to that obtained from running the functions from above. 

A plot of the decoded states is shown below.

```{r warning=F, message=F}
plot(eq_fit1)
```

### Fitting a Poisson-HMM by the EM Algorithm 

**Note:** See [EM Algorithm (Baum-Welch)](#em)

Alternatively, we may obtain MLEs of the three-state Poisson HMM by the EM algorithm. 

**E-Step** Replace all quantities $v_jk (t)$, $u_j (t)$ by their conditional expectations given the observations $\boldsymbol{x}^(T)$ and current parameter estimates:

$$\hat{u}_j = \Pr (C_t = j|\boldsymbol{x}^{(T)}) = \frac{\alpha_t (j) \beta_t (j)}{L_T}$$

$${\hat{v}}_{jk} (t) = \Pr (C_{t-1} = j, C_t = k| \boldsymbol{x}^{(T)}) \frac{\alpha_{t-1} (j) \gamma_{jk} p_k (x_t) \beta_t (k)}{L_T}$$


**Step 1**: Compute the forward and backward probabilities.

We use the same forward algorithm from the above and apply the backward algorithm `pois.HMM.lbackward`. 

The general algorithm is:

$\boldsymbol{\phi}_T \leftarrow \frac{\boldsymbol{1}}{w_T}$; $l \leftarrow \log w_T$

for $t = T-1, \dots, 1$

\begin{align}
\boldsymbol{v} &\leftarrow \boldsymbol{\Gamma P} (x_{t+1}) \boldsymbol{\phi_{t+1}}\\
l &\leftarrow l + \log \boldsymbol{v}\\
u &\leftarrow \boldsymbol{v 1'}\\
\boldsymbol{\phi}_t &\leftarrow \frac{\boldsymbol{v}}{u}\\
l & \leftarrow l + \log u
\end{align}


```{r}
pois.HMM.lbackward<-function(x,mod){
 n          <- length(x)
 m          <- mod$m
 lbeta      <- matrix(NA,m,n)
 lbeta[,n]  <- rep(0,m)
 
 # For t = T
 foo        <- rep(1/m,m)
 lscale     <- log(m)
 
 # For t = (T-1),..., 1
 for (i in (n-1):1){
    foo        <- mod$gamma%*%(dpois(x[i+1],mod$lambda)*foo)
    lbeta[,i]  <- log(foo)+lscale
    sumfoo     <- sum(foo)
    foo        <- foo/sumfoo
    lscale     <- lscale+log(sumfoo)
  }
 return(lbeta)
}
```

**Note:** `pois.HMM.lbackward` takes in the same arguments as the `pois.HMM.lforward`. 

**Step 2:** Replace $u_j(t)$ by $\hat{u}_j (t)$ and  $v_{jk}(t)$ by $\hat{v}_{jk} (t)$.

```{r}
uhat <- function(x, mod){
  n <- length(x)
  m <- mod$m
  
  # Forward probabilities
  la <- pois.HMM.lforward(x, mod)
  
  # Backward probabilities
  lb <- pois.HMM.lbackward(x, mod)
  
  c <- max(la[,n])
  
  # L_T (with some constant amount)
  llk <- c + log(sum(exp(la[,n] - c)))
  
  # Initialize
  stateprobs <- matrix(NA, ncol=n, nrow=m)
  
  ## (a_t(j) b_t (j))/L_T 
  for(i in 1:n){
    stateprobs[,i] <- exp(la[, i] + lb[, i] - llk)
  }
  return(stateprobs)
}


vhat <- function(x, mod){
  n <- length(x)
  m <- mod$m
  
  # Forward probabilities
  la <- pois.HMM.lforward(x, mod)
  
  # Backward probabilities
  lb <- pois.HMM.lbackward(x, mod)
  
  # tpm
  lgamma <- log(mod$gamma)
  c <- max(la[, n])
  
  # L_T (with some constant amount)
  llk <- c + log(sum(exp(la[, n] - c)))
  v <- array(0, dim=c(m, m, n))
  px <- matrix(NA, nrow=mod$m, ncol=n)
  
  for(i in 1:n){
    px[, i] <- dpois(x[i], mod$lambda)
  }
  lpx <- log(px)
  
  for(i in 2:n){
    for(j in 1:mod$m){
      for(k in 1:mod$m){
        v[j, k, i] <- exp(la[j, i-1] + lgamma[j, k] + lpx[k, i] + lb[k, i] - llk)
      }
    }
  }
  return(v)
}
```


**Note:** The term `c` in the code is a constant added to the log-forward and log-backward probabilities to prevent numerical underflow. 

**M-Step** Maximize the complete data log-likelihood with respect to the parameter estimates, with functions of the missing data replaced by their conditional expectations.

That is, maximize

\begin{align}
\log \left(\Pr(\boldsymbol{x}^{(T)}, \boldsymbol{c}^{(T)}) \right) 
&= \sum_{j=1}^m \hat{u}_j (1) + \log \delta_j + \sum_{j=1}^m \sum_{k=1}^m \left(\sum_{t=2}^T \hat{v}_{jk} (t)\right) \log \gamma_{jk} + \sum_{j=1}^m \sum_{t=1}^T \hat{u}_j (t) \log p_j (x_t)
\end{align}

with respect to $\boldsymbol{\theta} = (\boldsymbol{\delta}, \boldsymbol{\Gamma}, \boldsymbol{\lambda})$. 

The maximizations of the parameters are given by 

```{r}
E_delta <- function(uhat){
  d <- uhat[, 1]
  return(d)
}


E_gamma <- function(vhat, mod){
  num <- apply(vhat, MARGIN=c(1, 2), FUN=sum)
  denom <- rowSums(num)
  g <- num/denom
  return(g)
}

E_pois_lambda <- function(x,mod,uhat){
 n <- length(x)
 m <- mod$m
 xx <- matrix(rep(x, m), nrow=m, ncol=n, byrow=TRUE)
 lambda_hat <- rowSums(uhat*xx)/rowSums(uhat)
 return(lambda_hat)
}
```


The `BaumWelch` function performs the EM algorithm by taking initial model parameters as inputs, then repeatedly fitting until convergence or the maximum number of iterations is reached (by default `maxit=50`). 

```{r}
BaumWelch <- function(x, m, lambda0, gamma0, delta0, maxit = 50){
  # Initial parameters
  mod_old <- list(m = m, lambda = lambda0, gamma = gamma0, delta = delta0)
  iter <- 1
  while(iter < maxit){
    # E-step
    u_hat <- uhat(x, mod_old)
    v_hat <- vhat(x, mod_old)
    
    # M-step
    delta_new <- E_delta(u_hat)
    gamma_new <- E_gamma(v_hat, mod_old)
    lambda_new <- E_pois_lambda(x, mod_old, u_hat)
    
    mod_old <- list(m=m, lambda=lambda_new, gamma=gamma_new, delta=delta_new)
    iter = iter + 1
  }
  mod <- list(m=m, lambda=lambda_new, gamma=gamma_new, delta=delta_new)
  return(mod)
}
```

Hence, for the three-state Poisson-HMM 

```{r}
# Fitting a three-state HMM
BaumWelch(x, eq_init$m, eq_init$lambda, eq_init$gamma, delta = rep(1, eq_init$m)/eq_init$m)
```

### Forecasting, Decoding, and State Prediction

**Note:** See [Forecasting, Decoding, and State Prediction](#fdp).

#### Forecasting

The forecast probabilities $\Pr(X_{T+h} = x|\boldsymbol{X}^{(T)} = \boldsymbol{x}^{(T)})$ can be computed by 

```{r}
pois.HMM.forecast <- function(xf,h=1,x,mod)
{
 n        <- length(x)
 nxf      <- length(xf)
 dxf      <- matrix(0,nrow=h,ncol=nxf)
 foo      <- mod$delta*dpois(x[1],mod$lambda)
 sumfoo   <- sum(foo)
 lscale   <- log(sumfoo)
 foo      <- foo/sumfoo
 for (i in 2:n)
   {
   foo    <- foo%*%mod$gamma*dpois(x[i],mod$lambda)
   sumfoo <- sum(foo)
   lscale <- lscale+log(sumfoo)
   foo    <- foo/sumfoo
   }
  for (i in 1:h)
   {
   foo    <- foo%*%mod$gamma
   for (j in 1:mod$m) dxf[i,] <- dxf[i,] + foo[j]*dpois(xf,mod$lambda[j])
    }
 return(dxf)
}
```

**Note:** `pois.HMM.forecast` takes in arguments `xf` the range of $x$-values for the forecast probabilities, `h` the forecast horizon, `x` data, and  `mod` list consisting of `m` states, `lambda` means for the Poisson state-dependent distribution, and `delta` initial probabilities. 

For example, the forecast distributions for 1 to 4 years ahead with possible values up to 45 earthquakes is

```{r forecastfour, fig.cap="Forecast probabilities for the fitted three-state Poisson-HMM for 1 to 4 years ahead.", echo=F}
h<-4
xf<-0:45
forecasts<-pois.HMM.forecast(xf,h,x,mod3s)

par(mfrow=c(2,2),las=1)
for (i in 1:h)
  {
fc<-forecasts[i,]
plot(xf,fc,type="h",main=paste("Forecast distribution for", 2006+i),
xlim=c(0,max(xf)),ylim=c(0,0.12),xlab="count",ylab="probability",lwd=3)
}
```





#### Decoding

Local decoding can be computed using the forward and backward algorithms from above. Similar to computing $\hat{u}$ in the **E step** of the EM algorithm, the `pois.HMM.state_probs` computes the state probabilities then `pois.HMM.local_decoding` determines the most likely state at each time. 

```{r}
pois.HMM.state_probs <- function(x,mod)
{
 n          <- length(x)
 la         <- pois.HMM.lforward(x,mod)
 lb         <- pois.HMM.lbackward(x,mod)
 c          <- max(la[,n])
 llk        <- c+log(sum(exp(la[,n]-c)))
 stateprobs <- matrix(NA,ncol=n,nrow=mod$m)
 for (i in 1:n) stateprobs[,i]<-exp(la[,i]+lb[,i]-llk)
 return(stateprobs)
}

pois.HMM.local_decoding <- function(x,mod)
{
 n          <- length(x)
 stateprobs <- pois.HMM.state_probs(x,mod)
 ild        <- rep(NA,n)
 for (i in 1:n) ild[i]<-which.max(stateprobs[,i])
 ild
}
```

For example, local decoding for the three state Poisson-HMM is

```{r}
# Local decoding
(earthquake_local <- pois.HMM.local_decoding(x, eq_init))
```

```{r local, fig.cap="Local decoding for the three-state Poisson-HMM. The horizontal lines indicate the state-dependent means and the points indicate the decoded state at the given time.", echo=F}
eq_ldf <- cbind(eq_dat, m=earthquake_local)

eq_ldf <- eq_ldf %>%
  mutate(s = case_when(
    m == 1 ~ 10,
    m == 2 ~ 15,
    m == 3 ~ 30, 
  ))

{plot(eq_dat, type = "l", xlab="", ylab="Count", main="")
abline(h=10, col="blue")
abline(h=15, col="blue")
abline(h=30, col="blue")
points(x=eq_ldf$year, eq_ldf$s, col=eq_ldf$s, pch=16)
}
```

We can also perform local decoding using `momentuHMM`

```{r}
# Local decoding
## Compute state probabilities for each time step
state_probs <- stateProbs(eq_fit1)

## Select state that maximizes probability
local_vec        <- rep(NA,length(x))
for(i in 1:length(x)) local_vec[i] <- which.max(state_probs[i,])
local_vec


```

```{r comparelocal, fig.cap="Local decoding for the three-state Poisson-HMM. The horizontal lines indicate the state-dependent means and the points indicate the decoded state at the given time using  pois.HMM.local_decoding (purple) and momentuHMM (orange).", echo=F}

eq_ldf2 <- cbind(eq_dat, m2=local_vec)

eq_ldf2 <- eq_ldf2 %>%
  mutate(s2 = case_when(
    m2 == 1 ~ 10,
    m2 == 2 ~ 15,
    m2 == 3 ~ 30, 
  ))

eq_ldfs <- merge(eq_ldf, eq_ldf2)

{plot(eq_dat, type = "l", xlab="", ylab="Count", main="")
abline(h=10, col="blue")
abline(h=15, col="blue")
abline(h=30, col="blue")
points(x=eq_ldf$year, eq_ldfs$s, col="purple", pch=16)
points(x=eq_ldf$year, eq_ldfs$s2, col="orange", pch=16)
}
```

Global decoding can be computed using the Viterbi algorithm 

```{r}
pois.HMM.viterbi<-function(x,mod)
 {
 n              <- length(x)
 xi             <- matrix(0,n,mod$m)
 foo            <- mod$delta*dpois(x[1],mod$lambda)
 xi[1,]         <- foo/sum(foo)
 for (i in 2:n)
  {
  foo<-apply(xi[i-1,]*mod$gamma,2,max)*dpois(x[i],mod$lambda)
  xi[i,] <- foo/sum(foo)
  }
 iv<-numeric(n)
 iv[n]     <-which.max(xi[n,])
 for (i in (n-1):1)
   iv[i] <- which.max(mod$gamma[,iv[i+1]]*xi[i,])
 return(iv)
}
```

For example, global decoding for the three state Poisson-HMM is 

```{r}
(earthquake_global <- pois.HMM.viterbi(x, eq_init))
```

```{r global, fig.cap="Global decoding for the three-state Poisson-HMM.", echo=F}
eq_gdf <- cbind(eq_dat, m=earthquake_global)

eq_gdf <- eq_gdf %>%
  mutate(s = case_when(
    m == 1 ~ 10,
    m == 2 ~ 15,
    m == 3 ~ 30, 
  ))

{plot(eq_dat, type = "l", xlab="", ylab="Count", main="")
abline(h=10, col="blue")
abline(h=15, col="blue")
abline(h=30, col="blue")
points(x=eq_gdf$year, eq_gdf$s, col=eq_gdf$s, pch=16)
}
```

```{r comparelg, fig.cap="Global (green) and local (brown) decoding for the three-state Poisson-HMM.", echo=F}
{plot(eq_dat, type = "l", xlab="", ylab="Count", main="")
abline(h=10, col="blue")
abline(h=15, col="blue")
abline(h=30, col="blue")
points(x=eq_gdf$year, eq_ldf$s, col="brown", pch=16)
points(x=eq_gdf$year, eq_gdf$s, col="#4BB446" , pch=16)
}
```

The result of local and global decoding are very similar but not identical. 

Similarly, we can perform global decoding using `momentuHMM`

```{r, warning=F, message=F}
(global_vec <- viterbi(eq_fit1))
plotStates(eq_fit1)
```


```{r compareglobal, fig.cap="Global decoding for the three-state Poisson-HMM using  pois.HMM.gocal_decoding (purple) and momentuHMM (orange).", echo=F}

eq_gdf2 <- cbind(eq_dat, m2=global_vec)

eq_gdf2 <- eq_gdf2 %>%
  mutate(s2 = case_when(
    m2 == 1 ~ 10,
    m2 == 2 ~ 15,
    m2 == 3 ~ 30, 
  ))

eq_gdfs <- merge(eq_gdf, eq_gdf2)

{plot(eq_dat, type = "l", xlab="", ylab="Count", main="")
abline(h=10, col="blue")
abline(h=15, col="blue")
abline(h=30, col="blue")
points(x=eq_ldf$year, eq_gdfs$s, col="purple", pch=16)
points(x=eq_ldf$year, eq_gdfs$s2, col="orange", pch=16)
}
```

There appears to be some differences between the results obtained from our function and from `momentuHMM`. 

**Note:** In both cases, the MC does not assume stationarity and the initial distribution is $\boldsymbol{\delta} = (1/3, 1/3, 1/3)$. 


#### State Predictions

The state prediction for $h$ steps ahead can be computed by 

```{r}
pois.HMM.state_prediction <- function(h=1,x,mod)
{
 n          <- length(x)
 la         <- pois.HMM.lforward(x,mod)
 c          <- max(la[,n])
 llk        <- c+log(sum(exp(la[,n]-c)))
 statepreds <- matrix(NA,ncol=h,nrow=mod$m)
 foo <- exp(la[,n]-llk)
 for (i in 1:h){
  foo<-foo%*%mod$gamma
  statepreds[,i]<-foo
  }
 return(statepreds)
}
```

For example, the state predictions for $h=1, \dots, 5$ is

```{r}
pois.HMM.state_prediction(h=5, x, eq_init)
```

The stationary distribution is 

```{r}
# Compute stationary distribution
solve(t(diag(eq_init$m)-eq_init$gamma+1),rep(1,eq_init$m))
```

Notice that as $h \rightarrow \infty$, the state distribution approaches the stationary distribution. 


## Bayesian Inference in STAN

We can fit the three-state Poisson-HMM to the earthquake data using R-STAN. 

```{r message=F, warning=F}
library(rstan)
library(bayesplot)
stan_data <- list(T=dim(eq_dat)[1], m=3, x=eq_dat$V2)
```






```{r message=F, warning=F}
pois.HMM.stan <- 
'data{
int<lower=0> m;                       // number of states
int<lower=1> T;                       // length of sequence
int<lower=0> x[T];                    // observations
}

parameters{
simplex[m] Gamma[m];                  // tpm
positive_ordered[m] lambda;           // ordered mean of state-dependent (prevent label switching)
}

transformed parameters{
  matrix[m, m] ta;                    // tpm used to compute stationary distribution                 
  simplex[m] statdist;                // stationary distribution

  for(j in 1:m){
    for(i in 1:m){
      ta[i, j] = Gamma[i, j];
    }
  }

//compute stationary distribution 
statdist =  to_vector((to_row_vector(rep_vector(1.0, m))/
      (diag_matrix(rep_vector(1.0, m)) - ta + rep_matrix(1, m, m))));
}

model{
  // initialise
  vector[m] log_Gamma_tr[m];
  vector[m] lp;
  vector[m] lp_p1;

  // priors
  lambda ~ gamma(0.1, 0.01);

  // transpose tpm and take log of entries
  for(i in 1:m)
    for(j in 1:m)
      log_Gamma_tr[j, i] = log(Gamma[i, j]);
    
  // forward algorithm
  // for t = 1
  for(i in 1:m)
    lp[i] = log(statdist[i]) + poisson_lpmf(x[1]|lambda[i]);
  
  // loop over observations
  for(t in 2:T){
  // loop over states
    for (i in 1:m)
      lp_p1[i] = log_sum_exp(log_Gamma_tr[i] + lp) + poisson_lpmf(x[t]|lambda[i]);
      
      lp = lp_p1;
   }
   target += log_sum_exp(lp);
  }
'

```

We first run 2000 iterations for each for each of the 4 chains, with the first 1000 draws drawn during the warm-up phase

```{r}
pois.HMM.stanfit <- stan(model_code = pois.HMM.stan, data = stan_data, refresh=2000)
```





The posterior estimates are 

```{r}
print(pois.HMM.stanfit,digits_summary = 3)
```

```{r postl, fig.cap="Posterior distribution for the state-dependent means of the three-state Poisson-HMM.", echo=F}
post_lambda <- rstan::extract(pois.HMM.stanfit, pars=c("lambda"))

{hist(post_lambda[[1]][,1],main="",xlab=expression(lambda[1]))
hist(post_lambda[[1]][,2],main="",xlab=expression(lambda[2]))
hist(post_lambda[[1]][,3],main="",xlab=expression(lambda[3]))
}
```


```{r postd, fig.cap="Posterior distribution for the initial distribution of the three-state Poisson-HMM.", echo=F}
post_delta <- rstan::extract(pois.HMM.stanfit, pars=c("statdist"))

{hist(post_delta[[1]][,1],main="",xlab=expression(delta[1]))
hist(post_delta[[1]][,2],main="",xlab=expression(delta[2]))
hist(post_delta[[1]][,3],main="",xlab=expression(delta[3]))
}
```









































































