<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction to Hidden Markov Models | Hidden Markov Model Notes</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction to Hidden Markov Models | Hidden Markov Model Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="vinky-wang/HMM-Notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction to Hidden Markov Models | Hidden Markov Model Notes" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Vinky Wang" />


<meta name="date" content="2023-06-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="prelim.html"/>
<link rel="next" href="numerical.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">HMM Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>0.1</b> Usage</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#render-book"><i class="fa fa-check"></i><b>0.2</b> Render book</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#preview-book"><i class="fa fa-check"></i><b>0.3</b> Preview book</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="prelim.html"><a href="prelim.html"><i class="fa fa-check"></i><b>1</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prelim.html"><a href="prelim.html#mix"><i class="fa fa-check"></i><b>1.1</b> Independent Mixture Models</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="prelim.html"><a href="prelim.html#properties"><i class="fa fa-check"></i><b>1.1.1</b> Properties</a></li>
<li class="chapter" data-level="1.1.2" data-path="prelim.html"><a href="prelim.html#parameter-estimation"><i class="fa fa-check"></i><b>1.1.2</b> Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="prelim.html"><a href="prelim.html#mc"><i class="fa fa-check"></i><b>1.2</b> Markov Chains</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="prelim.html"><a href="prelim.html#probabilities"><i class="fa fa-check"></i><b>1.2.1</b> Probabilities</a></li>
<li class="chapter" data-level="1.2.2" data-path="prelim.html"><a href="prelim.html#stationary-distribution"><i class="fa fa-check"></i><b>1.2.2</b> Stationary Distribution</a></li>
<li class="chapter" data-level="1.2.3" data-path="prelim.html"><a href="prelim.html#tp"><i class="fa fa-check"></i><b>1.2.3</b> Transition Probabilities Estimation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="prelim.html"><a href="prelim.html#exercises"><i class="fa fa-check"></i><b>1.3</b> Exercises</a></li>
<li class="chapter" data-level="1.4" data-path="prelim.html"><a href="prelim.html#solutions"><i class="fa fa-check"></i><b>1.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introhmm.html"><a href="introhmm.html"><i class="fa fa-check"></i><b>2</b> Introduction to Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introhmm.html"><a href="introhmm.html#sdd"><i class="fa fa-check"></i><b>2.1</b> State-Dependent Distributions</a></li>
<li class="chapter" data-level="2.2" data-path="introhmm.html"><a href="introhmm.html#marginal-distributions"><i class="fa fa-check"></i><b>2.2</b> Marginal Distributions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introhmm.html"><a href="introhmm.html#univariate-case"><i class="fa fa-check"></i><b>2.2.1</b> Univariate Case</a></li>
<li class="chapter" data-level="2.2.2" data-path="introhmm.html"><a href="introhmm.html#bivariate"><i class="fa fa-check"></i><b>2.2.2</b> Bivariate Case</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introhmm.html"><a href="introhmm.html#moments"><i class="fa fa-check"></i><b>2.3</b> Moments</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introhmm.html"><a href="introhmm.html#univariate-case-1"><i class="fa fa-check"></i><b>2.3.1</b> Univariate Case</a></li>
<li class="chapter" data-level="2.3.2" data-path="introhmm.html"><a href="introhmm.html#bivariate-case"><i class="fa fa-check"></i><b>2.3.2</b> Bivariate Case</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introhmm.html"><a href="introhmm.html#lik"><i class="fa fa-check"></i><b>2.4</b> Likelihood of Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introhmm.html"><a href="introhmm.html#forsec"><i class="fa fa-check"></i><b>2.4.1</b> Forward Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introhmm.html"><a href="introhmm.html#exercises-1"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
<li class="chapter" data-level="2.6" data-path="introhmm.html"><a href="introhmm.html#solutions-1"><i class="fa fa-check"></i><b>2.6</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numerical.html"><a href="numerical.html"><i class="fa fa-check"></i><b>3</b> Numerical Maximization of the Likelihood</a>
<ul>
<li class="chapter" data-level="3.1" data-path="numerical.html"><a href="numerical.html#likscale"><i class="fa fa-check"></i><b>3.1</b> Scaling the Likelihood Computation</a></li>
<li class="chapter" data-level="3.2" data-path="numerical.html"><a href="numerical.html#reparam"><i class="fa fa-check"></i><b>3.2</b> Reparameterization to Avoid Constraints</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="numerical.html"><a href="numerical.html#transition-probabilities"><i class="fa fa-check"></i><b>3.2.1</b> Transition Probabilities</a></li>
<li class="chapter" data-level="3.2.2" data-path="numerical.html"><a href="numerical.html#parameters-of-state-dependent-distributions"><i class="fa fa-check"></i><b>3.2.2</b> Parameters of State-Dependent Distributions</a></li>
<li class="chapter" data-level="3.2.3" data-path="numerical.html"><a href="numerical.html#initial-probabilities"><i class="fa fa-check"></i><b>3.2.3</b> Initial Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numerical.html"><a href="numerical.html#startval"><i class="fa fa-check"></i><b>3.3</b> Strategies for Choosing Starting Values</a></li>
<li class="chapter" data-level="3.4" data-path="numerical.html"><a href="numerical.html#boot"><i class="fa fa-check"></i><b>3.4</b> Obtaining Standard Errors and Confidence Intervals</a></li>
<li class="chapter" data-level="3.5" data-path="numerical.html"><a href="numerical.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
<li class="chapter" data-level="3.6" data-path="numerical.html"><a href="numerical.html#solutions-2"><i class="fa fa-check"></i><b>3.6</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="fbalg.html"><a href="fbalg.html"><i class="fa fa-check"></i><b>4</b> The Forward and Backward Algorithm</a>
<ul>
<li class="chapter" data-level="4.1" data-path="fbalg.html"><a href="fbalg.html#forward-and-backward-probabilities"><i class="fa fa-check"></i><b>4.1</b> Forward and Backward Probabilities</a></li>
<li class="chapter" data-level="4.2" data-path="fbalg.html"><a href="fbalg.html#properties-of-hmms"><i class="fa fa-check"></i><b>4.2</b> Properties of HMMs</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="fbalg.html"><a href="fbalg.html#property"><i class="fa fa-check"></i><b>4.2.1</b> Property</a></li>
<li class="chapter" data-level="4.2.2" data-path="fbalg.html"><a href="fbalg.html#property-1"><i class="fa fa-check"></i><b>4.2.2</b> Property</a></li>
<li class="chapter" data-level="4.2.3" data-path="fbalg.html"><a href="fbalg.html#property-2"><i class="fa fa-check"></i><b>4.2.3</b> Property</a></li>
<li class="chapter" data-level="4.2.4" data-path="fbalg.html"><a href="fbalg.html#property-3"><i class="fa fa-check"></i><b>4.2.4</b> Property</a></li>
<li class="chapter" data-level="4.2.5" data-path="fbalg.html"><a href="fbalg.html#property-4"><i class="fa fa-check"></i><b>4.2.5</b> Property</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="fbalg.html"><a href="fbalg.html#forward-probabilities-as-joint-probabilities"><i class="fa fa-check"></i><b>4.3</b> Forward Probabilities as Joint Probabilities</a></li>
<li class="chapter" data-level="4.4" data-path="fbalg.html"><a href="fbalg.html#backward-probabilities-as-conditional-probabilities"><i class="fa fa-check"></i><b>4.4</b> Backward Probabilities as Conditional Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="em.html"><a href="em.html"><i class="fa fa-check"></i><b>5</b> Expectation-Maximization Algorithm (Baum-Welch)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="em.html"><a href="em.html#em-algorithm-general"><i class="fa fa-check"></i><b>5.1</b> EM Algorithm (General)</a></li>
<li class="chapter" data-level="5.2" data-path="em.html"><a href="em.html#em-algorithm-for-hmms"><i class="fa fa-check"></i><b>5.2</b> EM Algorithm (for HMMs)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="em.html"><a href="em.html#stationary-markov-chain"><i class="fa fa-check"></i><b>5.2.1</b> Stationary Markov Chain</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="em.html"><a href="em.html#exercises-3"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
<li class="chapter" data-level="5.4" data-path="em.html"><a href="em.html#solutions-3"><i class="fa fa-check"></i><b>5.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="fdp.html"><a href="fdp.html"><i class="fa fa-check"></i><b>6</b> Forecasting, Decoding, and State Prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="fdp.html"><a href="fdp.html#conditional-distribution"><i class="fa fa-check"></i><b>6.1</b> Conditional Distribution</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="fdp.html"><a href="fdp.html#as-mixtures-of-state-dependent-probabilities"><i class="fa fa-check"></i><b>6.1.1</b> As Mixtures of State-Dependent Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="fdp.html"><a href="fdp.html#forecast-distributions"><i class="fa fa-check"></i><b>6.2</b> Forecast Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="fdp.html"><a href="fdp.html#as-mixtures-of-state-dependent-probabilities-1"><i class="fa fa-check"></i><b>6.2.1</b> As Mixtures of State-Dependent Probabilities</a></li>
<li class="chapter" data-level="6.2.2" data-path="fdp.html"><a href="fdp.html#forecast-distribution-in-the-limit"><i class="fa fa-check"></i><b>6.2.2</b> Forecast Distribution in the Limit</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="fdp.html"><a href="fdp.html#decoding"><i class="fa fa-check"></i><b>6.3</b> Decoding</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="fdp.html"><a href="fdp.html#local-decoding"><i class="fa fa-check"></i><b>6.3.1</b> Local Decoding</a></li>
<li class="chapter" data-level="6.3.2" data-path="fdp.html"><a href="fdp.html#global-decoding"><i class="fa fa-check"></i><b>6.3.2</b> Global Decoding</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="fdp.html"><a href="fdp.html#state-prediction"><i class="fa fa-check"></i><b>6.4</b> State Prediction</a></li>
<li class="chapter" data-level="6.5" data-path="fdp.html"><a href="fdp.html#exercises-4"><i class="fa fa-check"></i><b>6.5</b> Exercises</a></li>
<li class="chapter" data-level="6.6" data-path="fdp.html"><a href="fdp.html#solutions-4"><i class="fa fa-check"></i><b>6.6</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html"><i class="fa fa-check"></i><b>7</b> Bayesian Inference for HMMs</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#reparameterization-to-avoid-label-switching"><i class="fa fa-check"></i><b>7.1</b> Reparameterization to Avoid Label Switching</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#gibbs-sampling-procedure"><i class="fa fa-check"></i><b>7.2</b> Gibbs Sampling Procedure</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#gen"><i class="fa fa-check"></i><b>7.2.1</b> Generating Sample Paths of the MC</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#decom"><i class="fa fa-check"></i><b>7.2.2</b> Decomposing the Observed Counts into Regime Contributions</a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#updating-the-parameters"><i class="fa fa-check"></i><b>7.2.3</b> Updating the Parameters</a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#repeat-the-above"><i class="fa fa-check"></i><b>7.2.4</b> Repeat the Above</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#exercises-5"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="eq.html"><a href="eq.html"><i class="fa fa-check"></i><b>8</b> Major Earthquake Analysis</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="eq.html"><a href="eq.html#fitting-a-poisson-mixture-distribution"><i class="fa fa-check"></i><b>8.0.1</b> Fitting a Poisson Mixture Distribution</a></li>
<li class="chapter" data-level="8.0.2" data-path="eq.html"><a href="eq.html#fitting-a-poisson-hmm-by-numerical-maximization"><i class="fa fa-check"></i><b>8.0.2</b> Fitting a Poisson-HMM by Numerical Maximization</a></li>
<li class="chapter" data-level="8.0.3" data-path="eq.html"><a href="eq.html#fitting-a-poisson-hmm-by-the-em-algorithm"><i class="fa fa-check"></i><b>8.0.3</b> Fitting a Poisson-HMM by the EM Algorithm</a></li>
<li class="chapter" data-level="8.0.4" data-path="eq.html"><a href="eq.html#forecasting-decoding-and-state-prediction"><i class="fa fa-check"></i><b>8.0.4</b> Forecasting, Decoding, and State Prediction</a></li>
<li class="chapter" data-level="8.1" data-path="eq.html"><a href="eq.html#bayesian-inference-in-stan"><i class="fa fa-check"></i><b>8.1</b> Bayesian Inference in STAN</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>9</b> Appendix</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Hidden Markov Model Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introhmm" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Hidden Markov Models<a href="introhmm.html#introhmm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A <strong>hidden Markov model</strong> <span class="math inline">\(\{X_t: t \in \mathbb{N}\}\)</span> can be summarized by</p>
<p><span class="math display" id="eq:hmmh">\[\begin{align}
\Pr(C_t|\boldsymbol{C}^{(t-1)})
&amp;= \Pr(C_t|C_{t-1}) \qquad{t = 2, 3, \dots}
\tag{2.1}
\end{align}\]</span></p>
<p><span class="math display" id="eq:hmmo">\[\begin{align}
\Pr(X_t|\boldsymbol{X}^{(t-1)}, \boldsymbol{C}^{(t)})
&amp;= \Pr(X_t|C_t) \qquad{t \in \mathbb{N}}
\tag{2.2}
\end{align}\]</span></p>
<p>where</p>
<p><a href="introhmm.html#eq:hmmh">(2.1)</a> is the unobserved parameter process, which satisfies the Markov property</p>
<p><a href="introhmm.html#eq:hmmo">(2.2)</a> is the observed state-dependent process, which are conditionally independent on past and future observations and states</p>
<div id="sdd" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> State-Dependent Distributions<a href="introhmm.html#sdd" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>state-dependent distributions</strong> of observation <span class="math inline">\(X_t\)</span> given that the Markov chain is in state <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span> are the probability mass (pmf) or density (pdf) functions</p>
<p><span class="math display">\[p_i (x) = \Pr(X_t = x|C_t = i) \qquad{\text{for } i = 1, \dots, m}\]</span></p>
<p>or in matrix notation,</p>
<p><span class="math display">\[
\boldsymbol{P} (x) =
\begin{pmatrix}
p_1 (x) &amp; &amp; 0\\
&amp; \ddots &amp; \\
0 &amp; &amp; p_m(x)
\end{pmatrix}
\]</span></p>
<p><strong>Note:</strong> The remaining sections assume discrete observations <span class="math inline">\(X_t\)</span>, however the continuous case is analagous.</p>
</div>
<div id="marginal-distributions" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Marginal Distributions<a href="introhmm.html#marginal-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="univariate-case" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Univariate Case<a href="introhmm.html#univariate-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Define <span class="math inline">\(u_i (t) = \Pr (C_t = i)\)</span> for <span class="math inline">\(t = 1, \dots, T\)</span>.</p>
<p>The marginal distribution of <span class="math inline">\(X_t\)</span> where the MC is homogenous, but not necessarily stationary is given by</p>
<p><span class="math display">\[\begin{align}
\Pr (X_t = x)
&amp;= \sum_{i=1}^m \Pr (C_t = i) \Pr(X_t = x|C_t = i)\\
&amp;= \sum_{i=1}^m u_i (t) p_i (x)
\end{align}\]</span></p>
<p>or in matrix notation,</p>
<p><span class="math display">\[\begin{align}
\Pr(X_t = x)
&amp;= (u_1(t), \dots, u_m(t))
\begin{pmatrix}
p_1 (x) &amp; &amp; 0\\
&amp; \ddots &amp; \\
0 &amp; &amp; p_m(x)
\end{pmatrix}
\begin{pmatrix}
1\\
\vdots\\
1
\end{pmatrix}\\
&amp;= \boldsymbol{u} (t) \boldsymbol{P} (x) \boldsymbol{1&#39;}
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\boldsymbol{u} (t+1) = \boldsymbol{u} (t) \boldsymbol{\Gamma}\)</span> (Equation <a href="prelim.html#eq:post">(1.7)</a>), it follows that <span class="math inline">\(\boldsymbol{u} (t) = \boldsymbol{u} (1) \boldsymbol{\Gamma}^{(t-1)}\)</span>, so</p>
<p><span class="math display">\[\begin{equation}
\Pr(X_t = x) = \boldsymbol{u} (1) \boldsymbol{\Gamma}^{t-1} \boldsymbol{P} (x) \boldsymbol{1&#39;}
\end{equation}\]</span></p>
<p>The marginal distribution of <span class="math inline">\(X_t\)</span> if the MC is stationary is given by</p>
<p><span class="math display">\[\begin{equation}
\Pr(X_t = x) = \boldsymbol{\delta P} (x) \boldsymbol{1&#39;}
\end{equation}\]</span></p>
<p><strong>Note:</strong> Since a MC with stationary distribution <span class="math inline">\(\boldsymbol{\delta}\)</span> such that <span class="math inline">\(\boldsymbol{\delta \Gamma} = \boldsymbol{\delta} \text{ and } \boldsymbol{\delta 1&#39;} = 1\)</span> implies that <span class="math inline">\(\boldsymbol{\delta \Gamma}^{t-1} = \boldsymbol{\delta}\)</span> for all <span class="math inline">\(t \in \mathbb{N}\)</span>.</p>
</div>
<div id="bivariate" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Bivariate Case<a href="introhmm.html#bivariate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The bivariate distribution of <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+k}\)</span> where the MC is homogenous, but not necessarily stationary is given by</p>
<p><span class="math display">\[\begin{align}
\Pr(X_t = v, X_{t+k} = w)
&amp;= \sum_{i=1}^m \sum_{j=1}^m \Pr (X_t = v, X_{t+k} = w, C_t = i, C_{t+k} = j)\\
&amp;= \sum_{i=1}^m \sum_{j=1}^m \Pr(C_t = i) \Pr(X_t = v|C_t = i) \Pr(C_{t+k} = j|C_t = i) \Pr(X_{t+k} = w|C_{t+k} = j)\\
&amp;= \sum_{i=1}^m \sum_{j=1}^m u_i (t) p_i (v) \gamma_{ij} (k) p_j (w)
\end{align}\]</span></p>
<p>or in matrix notation</p>
<p><span class="math display">\[\begin{align}
\Pr(X_t = v, X_{t+k} = w) &amp;= \boldsymbol{u} (t) \boldsymbol{P} (v) \boldsymbol{\Gamma}^k \boldsymbol{P} (w) \boldsymbol{1&#39;}
\qquad{\text{where } \gamma_{ij} (k) \text{ denotes the (i, j) element of } \boldsymbol{\Gamma}^k}
\end{align}\]</span></p>
<p>The marginal distribution of <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+k}\)</span> if the MC is stationary is given by
<span class="math display">\[\begin{align}
\Pr(X_t = v, X_{t+k} = w) &amp;= \boldsymbol{\delta P} (v) \boldsymbol{\Gamma}^k \boldsymbol{P} (w) \boldsymbol{1&#39;}
\end{align}\]</span></p>
<p><strong>Note:</strong> The above follows from Equation <a href="#eq:directed">(<strong>??</strong>)</a> in which <span class="math inline">\(\Pr(X_t, X_{t+k}, C_{t}, C_{t+k}) = \Pr (C_t) \Pr(X_t|C_t) \Pr(C_{t+k}|C_t) \Pr(X_{t+k}|C_{t+k})\)</span>.</p>
</div>
</div>
<div id="moments" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Moments<a href="introhmm.html#moments" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="univariate-case-1" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Univariate Case<a href="introhmm.html#univariate-case-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(g\)</span> be any functions for which the relevant state-dependent expectations exist.</p>
<p>The state-dependent expectation of <span class="math inline">\(X_t\)</span> where the MC is homogenous, but not necessarily stationary is given by</p>
<p><span class="math display">\[\begin{align}
E(g(X_t))
&amp;= \sum_{i=1}^m E(g(X_t)|C_t = i) \Pr(C_t = i)\\
&amp;= \sum_{i=1}^m u_i (t) E(g(X_t)|C_t = i)
\end{align}\]</span></p>
<p>and in the stationary case,</p>
<p><span class="math display">\[\begin{align}
E(X_t)
&amp;= \sum_{i=1}^m E(X_t|C_t = i) \Pr(C_t = i)\\
&amp;= \sum_{i=1}^m u_i (t) E(X_t|C_t = i)
\end{align}\]</span></p>
<p>and in the stationary case,</p>
<p><span class="math display">\[\begin{align}
E(g(X_t)) = \sum_{i=1}^m \delta_i E(g(X_t)|C_t = i)
\end{align}\]</span></p>
</div>
<div id="bivariate-case" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Bivariate Case<a href="introhmm.html#bivariate-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The state-dependent expectation of <span class="math inline">\(X_t\)</span> and <span class="math inline">\(X_{t+k}\)</span> where the MC is homogenous, but not necessarily stationary is given by</p>
<p><span class="math display">\[\begin{align}
E(g(X_t, X_{t+k}))
&amp;= \sum_{i, j=1}^m E(g(X_t, X_{t+k})|C_t = i, C_{t+k} = j) \Pr(C_t = i) \Pr(C_{t+k}=j|C_t = i)\\
&amp;= \sum_{i, j=1}^m E(g(X_t, X_{t+k})|C_t = i, C_{t+k} = j) u_i(t)\gamma_{ij} (k)
\end{align}\]</span></p>
<p>and in the stationary case,</p>
<p><span class="math display">\[\begin{align}
E(g(X_t, X_{t+k})) = \sum_{i, j=1}^m E(g(X_t, X_{t+k})|C_t = i, C_{t+k} = j) \delta_i \gamma_{ij} (k)
\end{align}\]</span></p>
<p>If <span class="math inline">\(g\)</span> is factorizable as <span class="math inline">\(g(X_t, X_{t+k}) = g_1 (X_t) g_2 (X_{t+k})\)</span>, then the above becomes</p>
<p><span class="math display">\[E(g(X_t, X_{t+k})) = \sum_{i, j=1}^m E(g_1(X_t)|C_t = i)E(g_2(X_{t+k})|C_{t+k}=j) \delta_i \gamma_{ij} (k)\]</span></p>
</div>
</div>
<div id="lik" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Likelihood of Hidden Markov Models<a href="introhmm.html#lik" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="proposition">
<p><span id="prp:likelihood" class="proposition"><strong>Proposition 2.1  </strong></span>The likelihood is given by</p>
</div>
<p><span class="math display" id="eq:hmmlik">\[\begin{equation}
L_T = \boldsymbol{\delta P} (x_1) \boldsymbol{\Gamma P} (x_2) \boldsymbol{\Gamma} \cdots \boldsymbol{\Gamma P} (x_T)
\boldsymbol{1&#39;}
\tag{2.3}
\end{equation}\]</span></p>
<p><em>If <span class="math inline">\(\boldsymbol{\delta}\)</span>, the distribution of <span class="math inline">\(C_1\)</span>, is the stationary distribution of the Markov chain, then in addition</em></p>
<p><span class="math display" id="eq:hmmlikstat">\[\begin{equation}
L_T = \boldsymbol{\delta \Gamma P} (x_1) \boldsymbol{\Gamma P} (x_2) \boldsymbol{\Gamma} \cdots \boldsymbol{\Gamma P} (x_T)
\boldsymbol{1&#39;}
\tag{2.4}
\end{equation}\]</span></p>
<p><em>Proof.</em></p>
<p><span class="math display">\[\begin{align}
L_T
&amp;= \Pr(\boldsymbol{X}^{(T)} = \boldsymbol{x}^{(T)})\\
&amp;= \sum_{c_1, c_2, \dots, c_T = 1}^m \Pr(\boldsymbol{X}^{(T)} = \boldsymbol{x}^{(T)}, \boldsymbol{C}^{(T)} = \boldsymbol{c}^{(T)})
\qquad{\text{by LOTP}}\\
&amp;= \sum_{c_1, c_2, \dots, c_T = 1}^m \Pr(C_1 = c_1) \prod_{k=2}^T \Pr(C_k = c_k|C_{k-1}=c_{k-1}) \prod_{k=1}^T \Pr(X_k = x_k|C_k = c_k)
\qquad{\text{by Equation (10.1)}}\\
&amp;= \sum_{c_1, c_2, \dots, c_T = 1}^m \left(\delta_{c_1} \gamma_{c_1, c_2} \gamma_{c_2, c_3} \cdots \gamma_{c_{T-1}, c_T} \right) \left( p_{c_1} (x_1) p_{c_2} (x_2) \cdots p_{c_T} (x_T) \right)\\
&amp;= \sum_{c_1, \dots, c_T = 1}^m \delta_{c_1} p_{c_1} (x_1) \gamma_{c_1, c_2} p_{c_2} (x_2) \gamma_{c_2, c_3} \cdots \gamma_{c_{T-1}, c_T} p_{c_T} (x_T)\\
&amp;= \boldsymbol{\delta P} (x_1) \boldsymbol{\Gamma P} (x_2) \boldsymbol{\Gamma} \cdots \boldsymbol{\Gamma P} (x_T) \boldsymbol{1&#39;}
\end{align}\]</span></p>
<p>If <span class="math inline">\(\boldsymbol{\delta}\)</span> is the stationary distribution, then <span class="math inline">\(\boldsymbol{\delta P} (x_1) = \boldsymbol{\delta \Gamma P} (x_1)\)</span>.</p>
<p><strong>Note:</strong> The likelihood consists of a sum of <span class="math inline">\(m^T\)</span> terms, each of which is a product of <span class="math inline">\(2T\)</span> factors, which would require <span class="math inline">\(\mathcal{O} (Tm^T)\)</span> operations. For example, the likelihood from the <a href="introhmm.html#bivariate">Bivariate Distributions section</a> consists of <span class="math inline">\(m^2\)</span> terms, each of which is a product of <span class="math inline">\(2 \times 2\)</span> factors. The recursive nature of the likelihood drastically reduces the model complexity and can be evaluated by the <strong>forward algorithm</strong> which performs <span class="math inline">\(\mathcal{O} (Tm^2)\)</span> operations. This makes numerical maximization feasible as shown in the next chapter.</p>
<div id="forsec" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Forward Algorithm<a href="introhmm.html#forsec" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For <span class="math inline">\(t = 1, 2, \dots, T\)</span></p>
<p><span class="math display" id="eq:forward">\[\begin{align}
\boldsymbol{\alpha}_t
&amp;= \boldsymbol{\delta P} (x_1) \boldsymbol{\Gamma P} (x_2) \boldsymbol{\Gamma} \cdots \boldsymbol{\Gamma P} (x_T)\\
&amp;= \boldsymbol{\delta P} (x_1) \prod_{s=2}^t \boldsymbol{\Gamma P} (x_s)
\tag{2.5}
\end{align}\]</span></p>
<p>with the convention that an empty product is the identity matrix.</p>
<p>It follows from this definition that</p>
<p><span class="math display">\[\begin{equation}
L_T = \boldsymbol{\alpha}_T \boldsymbol{1&#39;}
\qquad{ \text{ and }}
\qquad{}
\boldsymbol{\alpha}_t = \boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P} (x_t) \text{ for } t \geq 2
\end{equation}\]</span></p>
<p>For a homogenous but not necessarily stationary MC, the likelihood can be computed as <span class="math inline">\(L_T = \boldsymbol{\alpha}_T \boldsymbol{1&#39;}\)</span> where</p>
<p><span class="math display">\[\boldsymbol{\alpha}_1 = \boldsymbol{\delta P} (x_1)\]</span></p>
<p><span class="math display">\[\boldsymbol{\alpha}_t = \boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P} (x_t) \qquad{\text{for } t = 2, 3, \dots, T}\]</span></p>
<p>and in the stationary case,</p>
<p><span class="math display">\[\boldsymbol{\alpha}_0 = \boldsymbol{\delta}\]</span></p>
<p><span class="math display">\[\boldsymbol{\alpha}_t = \boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P} (x_t) \qquad{\text{for } t = 1, 2, \dots, T}\]</span></p>
</div>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Exercises<a href="introhmm.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Consider a stationary two-state Poisson-HMM with parameters</li>
</ol>
<p><span class="math display">\[\boldsymbol{\Gamma} =
  \begin{pmatrix}
  0.1 &amp; 0.9\\
  0.4 &amp; 0.6
  \end{pmatrix} \qquad{} \boldsymbol{\lambda} = (1, 3)\]</span></p>
<p><span class="math inline">\(\qquad{}\)</span> In each of the following ways, compute the probability that the first three observations from this
<span class="math inline">\(\qquad{}\)</span> model are 0, 2, 1.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Consider all possible sequences of the states of the Markov chain that could have occured. Compute the probability of each sequence, and the probability of the observations given each sequence.</p></li>
<li><p>Apply the formula <span class="math inline">\(\Pr(X_1 = 0, X_2 = 2, X_3 = 1) = \boldsymbol{\delta P} (0) \boldsymbol{\Gamma P} (2) \boldsymbol{\Gamma P} (1) \boldsymbol{1&#39;}\)</span>, where</p></li>
</ol>
<p><span class="math display">\[\begin{align}
  \boldsymbol{P} (x)
  &amp;=
  \begin{pmatrix}
  \frac{\lambda_1^s e^{- \lambda_1}}{s!} &amp; 0\\
  0 &amp; \frac{\lambda_2^s e^{- \lambda_2}}{s!}
  \end{pmatrix}\\
  &amp;=
  \begin{pmatrix}
  \frac{1^s e^{-1}}{s!} &amp; 0\\
  0 &amp; \frac{3^s e^{-3}}{s!}
  \end{pmatrix}
  \end{align}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><ol style="list-style-type: lower-alpha">
<li>Consider the vector <span class="math inline">\(\boldsymbol{\alpha}_t = (\alpha_1(1), \dots, \alpha_t (m))\)</span> defined by
<span class="math display">\[\alpha_t(j) = \Pr(\boldsymbol{X}^{(t)} = \boldsymbol{x}^{(t)}, C_t = j), \qquad{j = 1, 2, \dots, m}\]</span></li>
</ol>
<p><span class="math inline">\(\qquad{}\)</span> Use conditional probability and the conditional independence assumptions to show that</p>
<p><span class="math display" id="eq:ascalar">\[\begin{equation}
  \alpha_t(j) = \sum_{i=1}^m \alpha_{t-1} (i) \gamma_{ij} p_j (x_t)
  \tag{2.6}
\end{equation}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Verify that the result from (a), written in matrix notation, yields the forward recursion
<span class="math display">\[\boldsymbol{\alpha}_t = \boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P} (x_t) \qquad{t = 2, \dots, T}\]</span></p></li>
<li><p>Hence derive the matrix expression for the likelihood.</p></li>
</ol></li>
</ol>
</div>
<div id="solutions-1" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Solutions<a href="introhmm.html#solutions-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Question 1</strong></p>
<p><strong>Part a</strong></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="introhmm.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up -----------------------------------------------------------------------</span></span>
<span id="cb11-2"><a href="introhmm.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Load package</span></span>
<span id="cb11-3"><a href="introhmm.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb11-4"><a href="introhmm.html#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="introhmm.html#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Recall statdist function in chapter 1</span></span>
<span id="cb11-6"><a href="introhmm.html#cb11-6" aria-hidden="true" tabindex="-1"></a>statdist <span class="ot">=</span> <span class="cf">function</span>(Gamma){</span>
<span id="cb11-7"><a href="introhmm.html#cb11-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># number of components</span></span>
<span id="cb11-8"><a href="introhmm.html#cb11-8" aria-hidden="true" tabindex="-1"></a>  m <span class="ot">=</span> <span class="fu">nrow</span>(Gamma)</span>
<span id="cb11-9"><a href="introhmm.html#cb11-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-10"><a href="introhmm.html#cb11-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># identity matrix (mxm)</span></span>
<span id="cb11-11"><a href="introhmm.html#cb11-11" aria-hidden="true" tabindex="-1"></a>  I <span class="ot">=</span> <span class="fu">diag</span>(m)</span>
<span id="cb11-12"><a href="introhmm.html#cb11-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># ones matrix (mxm)</span></span>
<span id="cb11-13"><a href="introhmm.html#cb11-13" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(<span class="dv">1</span>,m<span class="sc">*</span>m), <span class="at">nrow=</span>m,<span class="at">ncol=</span>m)</span>
<span id="cb11-14"><a href="introhmm.html#cb11-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># vector of ones (1xm)</span></span>
<span id="cb11-15"><a href="introhmm.html#cb11-15" aria-hidden="true" tabindex="-1"></a>  one_vec <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>, m)</span>
<span id="cb11-16"><a href="introhmm.html#cb11-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-17"><a href="introhmm.html#cb11-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># I - Gamma + U</span></span>
<span id="cb11-18"><a href="introhmm.html#cb11-18" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> I <span class="sc">-</span> Gamma <span class="sc">+</span> U</span>
<span id="cb11-19"><a href="introhmm.html#cb11-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-20"><a href="introhmm.html#cb11-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># delta(Im - Gamma + U) = 1</span></span>
<span id="cb11-21"><a href="introhmm.html#cb11-21" aria-hidden="true" tabindex="-1"></a>  delta <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X), one_vec)</span>
<span id="cb11-22"><a href="introhmm.html#cb11-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(delta)</span>
<span id="cb11-23"><a href="introhmm.html#cb11-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-24"><a href="introhmm.html#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="introhmm.html#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="introhmm.html#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Question 1 --------------------------------------------------------------------</span></span>
<span id="cb11-27"><a href="introhmm.html#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="introhmm.html#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters </span></span>
<span id="cb11-29"><a href="introhmm.html#cb11-29" aria-hidden="true" tabindex="-1"></a>Gamma_pois <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.4</span>, <span class="fl">0.9</span>, <span class="fl">0.6</span>), <span class="at">nrow=</span><span class="dv">2</span>, <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb11-30"><a href="introhmm.html#cb11-30" aria-hidden="true" tabindex="-1"></a>lambda_pois <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb11-31"><a href="introhmm.html#cb11-31" aria-hidden="true" tabindex="-1"></a>delta_pois <span class="ot">=</span> <span class="fu">statdist</span>(Gamma_pois) <span class="do">## assuming stationary</span></span>
<span id="cb11-32"><a href="introhmm.html#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="introhmm.html#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="introhmm.html#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="co"># All possible sequences</span></span>
<span id="cb11-35"><a href="introhmm.html#cb11-35" aria-hidden="true" tabindex="-1"></a>i <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">4</span>), <span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb11-36"><a href="introhmm.html#cb11-36" aria-hidden="true" tabindex="-1"></a>j <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>), <span class="dv">2</span>))</span>
<span id="cb11-37"><a href="introhmm.html#cb11-37" aria-hidden="true" tabindex="-1"></a>k <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="dv">4</span>))</span>
<span id="cb11-38"><a href="introhmm.html#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="introhmm.html#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="co"># values for X</span></span>
<span id="cb11-40"><a href="introhmm.html#cb11-40" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb11-41"><a href="introhmm.html#cb11-41" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb11-42"><a href="introhmm.html#cb11-42" aria-hidden="true" tabindex="-1"></a>x3 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb11-43"><a href="introhmm.html#cb11-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-44"><a href="introhmm.html#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="introhmm.html#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="introhmm.html#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Function that computes Pr(X=s|C=c)</span></span>
<span id="cb11-47"><a href="introhmm.html#cb11-47" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="cf">function</span>(lambda1, lambda2, index, s){</span>
<span id="cb11-48"><a href="introhmm.html#cb11-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(index<span class="sc">==</span><span class="dv">1</span>){</span>
<span id="cb11-49"><a href="introhmm.html#cb11-49" aria-hidden="true" tabindex="-1"></a>      ps <span class="ot">=</span> ((lambda1<span class="sc">^</span>s)<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span>lambda1))<span class="sc">/</span>(<span class="fu">factorial</span>(s))</span>
<span id="cb11-50"><a href="introhmm.html#cb11-50" aria-hidden="true" tabindex="-1"></a>      <span class="fu">return</span>(ps)</span>
<span id="cb11-51"><a href="introhmm.html#cb11-51" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb11-52"><a href="introhmm.html#cb11-52" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>{</span>
<span id="cb11-53"><a href="introhmm.html#cb11-53" aria-hidden="true" tabindex="-1"></a>      ps <span class="ot">=</span> ((lambda2<span class="sc">^</span>s)<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span>lambda2))<span class="sc">/</span>(<span class="fu">factorial</span>(s))</span>
<span id="cb11-54"><a href="introhmm.html#cb11-54" aria-hidden="true" tabindex="-1"></a>      <span class="fu">return</span>(ps)</span>
<span id="cb11-55"><a href="introhmm.html#cb11-55" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb11-56"><a href="introhmm.html#cb11-56" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-57"><a href="introhmm.html#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="introhmm.html#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="introhmm.html#cb11-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Pr(X=s|C=c)</span></span>
<span id="cb11-60"><a href="introhmm.html#cb11-60" aria-hidden="true" tabindex="-1"></a>pi0 <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb11-61"><a href="introhmm.html#cb11-61" aria-hidden="true" tabindex="-1"></a>pj2 <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb11-62"><a href="introhmm.html#cb11-62" aria-hidden="true" tabindex="-1"></a>pk1 <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb11-63"><a href="introhmm.html#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(ind <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(i)){</span>
<span id="cb11-64"><a href="introhmm.html#cb11-64" aria-hidden="true" tabindex="-1"></a>  pi0[ind] <span class="ot">=</span> <span class="fu">p</span>(lambda_pois[<span class="dv">1</span>], lambda_pois[<span class="dv">2</span>], i[ind], x1)</span>
<span id="cb11-65"><a href="introhmm.html#cb11-65" aria-hidden="true" tabindex="-1"></a>  pj2[ind] <span class="ot">=</span> <span class="fu">p</span>(lambda_pois[<span class="dv">1</span>], lambda_pois[<span class="dv">2</span>], j[ind], x2)</span>
<span id="cb11-66"><a href="introhmm.html#cb11-66" aria-hidden="true" tabindex="-1"></a>  pk1[ind] <span class="ot">=</span> <span class="fu">p</span>(lambda_pois[<span class="dv">1</span>], lambda_pois[<span class="dv">2</span>], k[ind], x3)</span>
<span id="cb11-67"><a href="introhmm.html#cb11-67" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-68"><a href="introhmm.html#cb11-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-69"><a href="introhmm.html#cb11-69" aria-hidden="true" tabindex="-1"></a><span class="co"># delta_i</span></span>
<span id="cb11-70"><a href="introhmm.html#cb11-70" aria-hidden="true" tabindex="-1"></a>delta_i <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(delta_pois[<span class="dv">1</span>], <span class="dv">4</span>), <span class="fu">rep</span>(delta_pois[<span class="dv">2</span>], <span class="dv">4</span>))</span>
<span id="cb11-71"><a href="introhmm.html#cb11-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-72"><a href="introhmm.html#cb11-72" aria-hidden="true" tabindex="-1"></a><span class="co"># gamma_ij</span></span>
<span id="cb11-73"><a href="introhmm.html#cb11-73" aria-hidden="true" tabindex="-1"></a>gamma_ij <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb11-74"><a href="introhmm.html#cb11-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-75"><a href="introhmm.html#cb11-75" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(ind <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(i)){</span>
<span id="cb11-76"><a href="introhmm.html#cb11-76" aria-hidden="true" tabindex="-1"></a>  gamma_ij[ind] <span class="ot">=</span> Gamma_pois[i[ind], j[ind]]</span>
<span id="cb11-77"><a href="introhmm.html#cb11-77" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-78"><a href="introhmm.html#cb11-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-79"><a href="introhmm.html#cb11-79" aria-hidden="true" tabindex="-1"></a><span class="co"># gamma_jk</span></span>
<span id="cb11-80"><a href="introhmm.html#cb11-80" aria-hidden="true" tabindex="-1"></a>gamma_jk <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb11-81"><a href="introhmm.html#cb11-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-82"><a href="introhmm.html#cb11-82" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(ind <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(i)){</span>
<span id="cb11-83"><a href="introhmm.html#cb11-83" aria-hidden="true" tabindex="-1"></a>  gamma_jk[ind] <span class="ot">=</span> Gamma_pois[j[ind], k[ind]]</span>
<span id="cb11-84"><a href="introhmm.html#cb11-84" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-85"><a href="introhmm.html#cb11-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-86"><a href="introhmm.html#cb11-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Table for likelihood computation</span></span>
<span id="cb11-87"><a href="introhmm.html#cb11-87" aria-hidden="true" tabindex="-1"></a>lktab <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(i, j, k, pi0, pj2, pk1, delta_i, gamma_ij, gamma_jk))</span>
<span id="cb11-88"><a href="introhmm.html#cb11-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-89"><a href="introhmm.html#cb11-89" aria-hidden="true" tabindex="-1"></a>lktab <span class="ot">&lt;-</span> lktab <span class="sc">%&gt;%</span></span>
<span id="cb11-90"><a href="introhmm.html#cb11-90" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">product =</span> pi0<span class="sc">*</span>pj2<span class="sc">*</span>pk1<span class="sc">*</span>delta_i<span class="sc">*</span>gamma_ij<span class="sc">*</span>gamma_jk) <span class="sc">%&gt;%</span></span>
<span id="cb11-91"><a href="introhmm.html#cb11-91" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.numeric, round, <span class="dv">4</span>)</span>
<span id="cb11-92"><a href="introhmm.html#cb11-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-93"><a href="introhmm.html#cb11-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood</span></span>
<span id="cb11-94"><a href="introhmm.html#cb11-94" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(lktab<span class="sc">$</span>product)</span></code></pre></div>
<pre><code>## [1] 0.0073</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="introhmm.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Global decoding</span></span>
<span id="cb13-2"><a href="introhmm.html#cb13-2" aria-hidden="true" tabindex="-1"></a>lktab[<span class="fu">which</span>(lktab<span class="sc">$</span>product <span class="sc">==</span> <span class="fu">max</span>(lktab<span class="sc">$</span>product)),] </span></code></pre></div>
<pre><code>##   i j k    pi0   pj2    pk1 delta_i gamma_ij gamma_jk product
## 3 1 2 1 0.3679 0.224 0.3679  0.3077      0.9      0.4  0.0034</code></pre>
<p>The sequence that maximizes the conditional probability is <span class="math inline">\(i = 1, j = 2, k=1\)</span></p>
<p><strong>Part b</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="introhmm.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># P(s) matrices</span></span>
<span id="cb15-2"><a href="introhmm.html#cb15-2" aria-hidden="true" tabindex="-1"></a>Ps <span class="ot">=</span> <span class="cf">function</span>(lambda1, lambda2, s){</span>
<span id="cb15-3"><a href="introhmm.html#cb15-3" aria-hidden="true" tabindex="-1"></a>  Ps_matrix <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(((lambda1<span class="sc">^</span>s)<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span>lambda1))<span class="sc">/</span>(<span class="fu">factorial</span>(s)),</span>
<span id="cb15-4"><a href="introhmm.html#cb15-4" aria-hidden="true" tabindex="-1"></a>                       <span class="dv">0</span>,</span>
<span id="cb15-5"><a href="introhmm.html#cb15-5" aria-hidden="true" tabindex="-1"></a>                       <span class="dv">0</span>,</span>
<span id="cb15-6"><a href="introhmm.html#cb15-6" aria-hidden="true" tabindex="-1"></a>                       ((lambda2<span class="sc">^</span>s)<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span>lambda2))<span class="sc">/</span>(<span class="fu">factorial</span>(s))), <span class="at">nrow=</span><span class="dv">2</span>, <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb15-7"><a href="introhmm.html#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(Ps_matrix)</span>
<span id="cb15-8"><a href="introhmm.html#cb15-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-9"><a href="introhmm.html#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="introhmm.html#cb15-10" aria-hidden="true" tabindex="-1"></a>P0 <span class="ot">=</span> <span class="fu">Ps</span>(lambda_pois[<span class="dv">1</span>], lambda_pois[<span class="dv">2</span>], <span class="dv">0</span>)</span>
<span id="cb15-11"><a href="introhmm.html#cb15-11" aria-hidden="true" tabindex="-1"></a>P2 <span class="ot">=</span> <span class="fu">Ps</span>(lambda_pois[<span class="dv">1</span>], lambda_pois[<span class="dv">2</span>], <span class="dv">2</span>)</span>
<span id="cb15-12"><a href="introhmm.html#cb15-12" aria-hidden="true" tabindex="-1"></a>P1 <span class="ot">=</span> <span class="fu">Ps</span>(lambda_pois[<span class="dv">1</span>], lambda_pois[<span class="dv">2</span>], <span class="dv">1</span>)</span>
<span id="cb15-13"><a href="introhmm.html#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="introhmm.html#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Row vector of ones</span></span>
<span id="cb15-15"><a href="introhmm.html#cb15-15" aria-hidden="true" tabindex="-1"></a>one_vec <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb15-16"><a href="introhmm.html#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="introhmm.html#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the formula</span></span>
<span id="cb15-18"><a href="introhmm.html#cb15-18" aria-hidden="true" tabindex="-1"></a>delta_pois<span class="sc">%*%</span>P0<span class="sc">%*%</span>Gamma_pois<span class="sc">%*%</span>P2<span class="sc">%*%</span>Gamma_pois<span class="sc">%*%</span>P1<span class="sc">%*%</span>one_vec</span></code></pre></div>
<pre><code>##            [,1]
## [1,] 0.00729174</code></pre>
<p><strong>Question 2</strong></p>
<p><strong>Part a</strong></p>
<p><span class="math display">\[\begin{align}
\alpha_t (j)
&amp;= \Pr(\boldsymbol{X}^{(t)} = \boldsymbol{x}^{(t)}, C_t = j)\\
&amp;= \Pr (X_t = x_t | \boldsymbol{X}^{(t-1)} = \boldsymbol{x}^{(t-1)}, C_t = j) \Pr(\boldsymbol{X}^{(t-1)} = \boldsymbol{x}^{(t-1)}, C_t = j)
\qquad{\text{by Chain rule}}\\
&amp;= \Pr(X_t = x_t |C_t = j) \Pr(\boldsymbol{X}^{(t-1)} = \boldsymbol{x}^{(t-1)}, C_t = j)
\qquad{\text{by conditional independence}}\\
&amp;= \Pr(X_t = x_t |C_t = j) \sum_{i=1}^m \Pr(C_t = j|\boldsymbol{X}^{(t-1)} = \boldsymbol{x}^{(t-1)}, C_{t-1} = i) \Pr(\boldsymbol{X}^{(t-1)} = \boldsymbol{x}^{(t-1)}, C_{t-1} = i)
\qquad{\text{by Law of Total Probability}}\\
&amp;= \Pr(X_t = x_t |C_t = j) \sum_{i=1}^m \Pr(C_t = j|C_{t-1} = i) \Pr(\boldsymbol{X}^{(t-1)} = \boldsymbol{x}^{(t-1)}, C_{t-1} = i)
\qquad{\text{by conditional independence}}\\
&amp;= \sum_{i=1}^m \Pr(\boldsymbol{X}^{(t-1)} = \boldsymbol{x}^{(t-1)}, C_{t-1} = i) \Pr(C_t = j|C_{t-1} = i) \Pr(X_t = x_t|C_t=j)\\
&amp;= \sum_{i=1}^m \alpha_{t-1} (i) \gamma_{ij} p_j (x_t)
\end{align}\]</span></p>
<p><strong>Part b</strong></p>
<p>Let <span class="math inline">\(j \in \mathbb{N}\)</span>. Then</p>
<p><span class="math display">\[\begin{align}
\left(\boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P} (x_t)\right)_{1j}\\
&amp;= \sum_{k=1}^m \left(\boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P}\right)_{1k} \left(\boldsymbol{\Gamma P} (x_t)\right)_{kj}\\
&amp;= \sum_{k=1}^m \sum_{i=1}^m \left(\boldsymbol{\alpha}_{t-1} \right)_{1i} \left(\boldsymbol{\Gamma}\right)_{ik} \left(\boldsymbol{P} (x_t)\right)_{kj}
\qquad{\text{since } \boldsymbol{P} (x_t) \text{ is a diagonal matrix so} \left(\boldsymbol{P} (x_t) \right)_{kj} = 0 for all k \neq j}\\
&amp;= \sum_{i=1}^m \boldsymbol{\alpha}_{t-1} (i) \gamma_{ij} p_j (x_t)
\end{align}\]</span></p>
<p>Since the <span class="math inline">\(j\)</span>-th entry of <span class="math inline">\(\boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P} (x_t)\)</span> is equal to the <span class="math inline">\(j\)</span>-th entry of <span class="math inline">\(\boldsymbol{\alpha}_t\)</span> for an arbitrary <span class="math inline">\(j \in \mathbb{N}\)</span>, we conclude that <span class="math inline">\(\boldsymbol{\alpha}_t = \boldsymbol{\alpha}_{t-1} \boldsymbol{\Gamma P} (x_t)\)</span>.</p>
<p><strong>Part c</strong></p>
<p>We will prove for <span class="math inline">\(t = 1, \dots, T\)</span> that <span class="math inline">\(\boldsymbol{\alpha}_t = \boldsymbol{\delta P} (x_1) \boldsymbol{\Gamma} \cdots \boldsymbol{\Gamma P} (x_t)\)</span> by induction on <span class="math inline">\(t\)</span>.</p>
<p>Base case:</p>
<p>If <span class="math inline">\(t=1\)</span>, then</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol{\alpha}_1
&amp;= (\alpha_1 (1), \dots, \alpha_1 (m))\\
&amp;= (\Pr(X_1 = x_1, C_1 = 1), \dots, \Pr(X_1 = x_1, C_1 = m))\\
&amp;= (\Pr(C_1 = 1) \Pr(X_1 = x_1|C_1 = 1), \dots, \Pr(C_1 = m) \Pr(X_1 = x_1|C_1 = m))\\
&amp;= (\delta_{c_1} p_1(x_1), \dots, \delta_{c_1} p_m (x_1))\\
&amp;= \boldsymbol{\delta P} (x_1)
\end{align*}\]</span></p>
<p>Inductive step:</p>
<p>Let <span class="math inline">\(t \in \{1, \dots, T\}\)</span>. Suppose <span class="math inline">\(\boldsymbol{\alpha}_t = \boldsymbol{\delta P} (x_1) \boldsymbol{\Gamma} \cdots \boldsymbol{\Gamma P} (x_t)\)</span>. Then</p>
<p><span class="math display">\[\begin{align*}
\boldsymbol{\alpha}_{t+1}
&amp;= (\alpha_{t+1} (1), \dots, \alpha_{t+1} (m))\\
&amp;= (\Pr(\boldsymbol{X}^{(t+1)} = \boldsymbol{x}^{(t+1)}, C_{t+1} = 1), \dots, \Pr(\boldsymbol{X}^{(t+1)} = \boldsymbol{x}^{(t+1)}, C_{t+1} = m))\\
&amp;= (\sum_{i=1}^m \alpha_t (i) \gamma_{i1} p_1 (x_{t+1}), \dots, \sum_{i=1}^m \alpha_t (i) \gamma_{im} p_m (x_{t+1}))
\qquad{\text{by part a}}\\
&amp;= \boldsymbol{\alpha}_t \boldsymbol{\Gamma P} (x_{t+1})
\qquad{\qquad{\text{by part b}}}\\
&amp;= \boldsymbol{\delta P} (x_1) \boldsymbol{\Gamma} \cdots  \boldsymbol{\Gamma P} (x_t) \boldsymbol{\Gamma P} (x_{t+1})
\qquad{\text{by the inductive hypothesis}}
\end{align*}\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prelim.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="numerical.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vinky-wang/HMM-Notes/edit/BRANCH/02-hmm-basics.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "https://github.com/vinky-wang/HMM-Notes/raw/BRANCH/02-hmm-basics.Rmd"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
