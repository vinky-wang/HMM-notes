<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Numerical Maximization of the Likelihood | Hidden Markov Model Notes</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Numerical Maximization of the Likelihood | Hidden Markov Model Notes" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="vinky-wang/HMM-Notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Numerical Maximization of the Likelihood | Hidden Markov Model Notes" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Vinky Wang" />


<meta name="date" content="2023-06-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introhmm.html"/>
<link rel="next" href="fbalg.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">HMM Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a></li>
<li class="chapter" data-level="2" data-path="prelim.html"><a href="prelim.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prelim.html"><a href="prelim.html#mix"><i class="fa fa-check"></i><b>2.1</b> Independent Mixture Models</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prelim.html"><a href="prelim.html#properties"><i class="fa fa-check"></i><b>2.1.1</b> Properties</a></li>
<li class="chapter" data-level="2.1.2" data-path="prelim.html"><a href="prelim.html#parameter-estimation"><i class="fa fa-check"></i><b>2.1.2</b> Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prelim.html"><a href="prelim.html#mc"><i class="fa fa-check"></i><b>2.2</b> Markov Chains</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prelim.html"><a href="prelim.html#probabilities"><i class="fa fa-check"></i><b>2.2.1</b> Probabilities</a></li>
<li class="chapter" data-level="2.2.2" data-path="prelim.html"><a href="prelim.html#stationary-distribution"><i class="fa fa-check"></i><b>2.2.2</b> Stationary Distribution</a></li>
<li class="chapter" data-level="2.2.3" data-path="prelim.html"><a href="prelim.html#tp"><i class="fa fa-check"></i><b>2.2.3</b> Transition Probabilities Estimation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prelim.html"><a href="prelim.html#exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="prelim.html"><a href="prelim.html#solutions"><i class="fa fa-check"></i><b>2.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introhmm.html"><a href="introhmm.html"><i class="fa fa-check"></i><b>3</b> Introduction to Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introhmm.html"><a href="introhmm.html#sdd"><i class="fa fa-check"></i><b>3.1</b> State-Dependent Distributions</a></li>
<li class="chapter" data-level="3.2" data-path="introhmm.html"><a href="introhmm.html#marginal-distributions"><i class="fa fa-check"></i><b>3.2</b> Marginal Distributions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="introhmm.html"><a href="introhmm.html#univariate-case"><i class="fa fa-check"></i><b>3.2.1</b> Univariate Case</a></li>
<li class="chapter" data-level="3.2.2" data-path="introhmm.html"><a href="introhmm.html#bivariate"><i class="fa fa-check"></i><b>3.2.2</b> Bivariate Case</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="introhmm.html"><a href="introhmm.html#moments"><i class="fa fa-check"></i><b>3.3</b> Moments</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="introhmm.html"><a href="introhmm.html#univariate-case-1"><i class="fa fa-check"></i><b>3.3.1</b> Univariate Case</a></li>
<li class="chapter" data-level="3.3.2" data-path="introhmm.html"><a href="introhmm.html#bivariate-case"><i class="fa fa-check"></i><b>3.3.2</b> Bivariate Case</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introhmm.html"><a href="introhmm.html#lik"><i class="fa fa-check"></i><b>3.4</b> Likelihood of Hidden Markov Models</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="introhmm.html"><a href="introhmm.html#forsec"><i class="fa fa-check"></i><b>3.4.1</b> Forward Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="introhmm.html"><a href="introhmm.html#exercises-1"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
<li class="chapter" data-level="3.6" data-path="introhmm.html"><a href="introhmm.html#solutions-1"><i class="fa fa-check"></i><b>3.6</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numerical.html"><a href="numerical.html"><i class="fa fa-check"></i><b>4</b> Numerical Maximization of the Likelihood</a>
<ul>
<li class="chapter" data-level="4.1" data-path="numerical.html"><a href="numerical.html#likscale"><i class="fa fa-check"></i><b>4.1</b> Scaling the Likelihood Computation</a></li>
<li class="chapter" data-level="4.2" data-path="numerical.html"><a href="numerical.html#reparam"><i class="fa fa-check"></i><b>4.2</b> Reparameterization to Avoid Constraints</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="numerical.html"><a href="numerical.html#transition-probabilities"><i class="fa fa-check"></i><b>4.2.1</b> Transition Probabilities</a></li>
<li class="chapter" data-level="4.2.2" data-path="numerical.html"><a href="numerical.html#parameters-of-state-dependent-distributions"><i class="fa fa-check"></i><b>4.2.2</b> Parameters of State-Dependent Distributions</a></li>
<li class="chapter" data-level="4.2.3" data-path="numerical.html"><a href="numerical.html#initial-probabilities"><i class="fa fa-check"></i><b>4.2.3</b> Initial Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numerical.html"><a href="numerical.html#startval"><i class="fa fa-check"></i><b>4.3</b> Strategies for Choosing Starting Values</a></li>
<li class="chapter" data-level="4.4" data-path="numerical.html"><a href="numerical.html#boot"><i class="fa fa-check"></i><b>4.4</b> Obtaining Standard Errors and Confidence Intervals</a></li>
<li class="chapter" data-level="4.5" data-path="numerical.html"><a href="numerical.html#exercises-2"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
<li class="chapter" data-level="4.6" data-path="numerical.html"><a href="numerical.html#solutions-2"><i class="fa fa-check"></i><b>4.6</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fbalg.html"><a href="fbalg.html"><i class="fa fa-check"></i><b>5</b> The Forward and Backward Algorithm</a>
<ul>
<li class="chapter" data-level="5.1" data-path="fbalg.html"><a href="fbalg.html#forward-and-backward-probabilities"><i class="fa fa-check"></i><b>5.1</b> Forward and Backward Probabilities</a></li>
<li class="chapter" data-level="5.2" data-path="fbalg.html"><a href="fbalg.html#properties-of-hmms"><i class="fa fa-check"></i><b>5.2</b> Properties of HMMs</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="fbalg.html"><a href="fbalg.html#property"><i class="fa fa-check"></i><b>5.2.1</b> Property</a></li>
<li class="chapter" data-level="5.2.2" data-path="fbalg.html"><a href="fbalg.html#property-1"><i class="fa fa-check"></i><b>5.2.2</b> Property</a></li>
<li class="chapter" data-level="5.2.3" data-path="fbalg.html"><a href="fbalg.html#property-2"><i class="fa fa-check"></i><b>5.2.3</b> Property</a></li>
<li class="chapter" data-level="5.2.4" data-path="fbalg.html"><a href="fbalg.html#property-3"><i class="fa fa-check"></i><b>5.2.4</b> Property</a></li>
<li class="chapter" data-level="5.2.5" data-path="fbalg.html"><a href="fbalg.html#property-4"><i class="fa fa-check"></i><b>5.2.5</b> Property</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="fbalg.html"><a href="fbalg.html#forward-probabilities-as-joint-probabilities"><i class="fa fa-check"></i><b>5.3</b> Forward Probabilities as Joint Probabilities</a></li>
<li class="chapter" data-level="5.4" data-path="fbalg.html"><a href="fbalg.html#backward-probabilities-as-conditional-probabilities"><i class="fa fa-check"></i><b>5.4</b> Backward Probabilities as Conditional Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="em.html"><a href="em.html"><i class="fa fa-check"></i><b>6</b> Expectation-Maximization Algorithm (Baum-Welch)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="em.html"><a href="em.html#em-algorithm-general"><i class="fa fa-check"></i><b>6.1</b> EM Algorithm (General)</a></li>
<li class="chapter" data-level="6.2" data-path="em.html"><a href="em.html#em-algorithm-for-hmms"><i class="fa fa-check"></i><b>6.2</b> EM Algorithm (for HMMs)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="em.html"><a href="em.html#stationary-markov-chain"><i class="fa fa-check"></i><b>6.2.1</b> Stationary Markov Chain</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="em.html"><a href="em.html#exercises-3"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
<li class="chapter" data-level="6.4" data-path="em.html"><a href="em.html#solutions-3"><i class="fa fa-check"></i><b>6.4</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="fdp.html"><a href="fdp.html"><i class="fa fa-check"></i><b>7</b> Forecasting, Decoding, and State Prediction</a>
<ul>
<li class="chapter" data-level="7.1" data-path="fdp.html"><a href="fdp.html#conditional-distribution"><i class="fa fa-check"></i><b>7.1</b> Conditional Distribution</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="fdp.html"><a href="fdp.html#as-mixtures-of-state-dependent-probabilities"><i class="fa fa-check"></i><b>7.1.1</b> As Mixtures of State-Dependent Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="fdp.html"><a href="fdp.html#forecast-distributions"><i class="fa fa-check"></i><b>7.2</b> Forecast Distributions</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="fdp.html"><a href="fdp.html#as-mixtures-of-state-dependent-probabilities-1"><i class="fa fa-check"></i><b>7.2.1</b> As Mixtures of State-Dependent Probabilities</a></li>
<li class="chapter" data-level="7.2.2" data-path="fdp.html"><a href="fdp.html#forecast-distribution-in-the-limit"><i class="fa fa-check"></i><b>7.2.2</b> Forecast Distribution in the Limit</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="fdp.html"><a href="fdp.html#decoding"><i class="fa fa-check"></i><b>7.3</b> Decoding</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="fdp.html"><a href="fdp.html#local-decoding"><i class="fa fa-check"></i><b>7.3.1</b> Local Decoding</a></li>
<li class="chapter" data-level="7.3.2" data-path="fdp.html"><a href="fdp.html#global-decoding"><i class="fa fa-check"></i><b>7.3.2</b> Global Decoding</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="fdp.html"><a href="fdp.html#state-prediction"><i class="fa fa-check"></i><b>7.4</b> State Prediction</a></li>
<li class="chapter" data-level="7.5" data-path="fdp.html"><a href="fdp.html#exercises-4"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
<li class="chapter" data-level="7.6" data-path="fdp.html"><a href="fdp.html#solutions-4"><i class="fa fa-check"></i><b>7.6</b> Solutions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html"><i class="fa fa-check"></i><b>8</b> Bayesian Inference for HMMs</a>
<ul>
<li class="chapter" data-level="8.1" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#reparameterization-to-avoid-label-switching"><i class="fa fa-check"></i><b>8.1</b> Reparameterization to Avoid Label Switching</a></li>
<li class="chapter" data-level="8.2" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#gibbs-sampling-procedure"><i class="fa fa-check"></i><b>8.2</b> Gibbs Sampling Procedure</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#gen"><i class="fa fa-check"></i><b>8.2.1</b> Generating Sample Paths of the MC</a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#decom"><i class="fa fa-check"></i><b>8.2.2</b> Decomposing the Observed Counts into Regime Contributions</a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#updating-the-parameters"><i class="fa fa-check"></i><b>8.2.3</b> Updating the Parameters</a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#repeat-the-above"><i class="fa fa-check"></i><b>8.2.4</b> Repeat the Above</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian-inference-for-hmms.html"><a href="bayesian-inference-for-hmms.html#exercises-5"><i class="fa fa-check"></i><b>8.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="eq.html"><a href="eq.html"><i class="fa fa-check"></i><b>9</b> Major Earthquake Analysis</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="eq.html"><a href="eq.html#fitting-a-poisson-mixture-distribution"><i class="fa fa-check"></i><b>9.0.1</b> Fitting a Poisson Mixture Distribution</a></li>
<li class="chapter" data-level="9.0.2" data-path="eq.html"><a href="eq.html#fitting-a-poisson-hmm-by-numerical-maximization"><i class="fa fa-check"></i><b>9.0.2</b> Fitting a Poisson-HMM by Numerical Maximization</a></li>
<li class="chapter" data-level="9.0.3" data-path="eq.html"><a href="eq.html#fitting-a-poisson-hmm-by-the-em-algorithm"><i class="fa fa-check"></i><b>9.0.3</b> Fitting a Poisson-HMM by the EM Algorithm</a></li>
<li class="chapter" data-level="9.0.4" data-path="eq.html"><a href="eq.html#forecasting-decoding-and-state-prediction"><i class="fa fa-check"></i><b>9.0.4</b> Forecasting, Decoding, and State Prediction</a></li>
<li class="chapter" data-level="9.1" data-path="eq.html"><a href="eq.html#bayesian-inference-in-stan"><i class="fa fa-check"></i><b>9.1</b> Bayesian Inference in STAN</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>10</b> Appendix</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Hidden Markov Model Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="numerical" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Numerical Maximization of the Likelihood<a href="numerical.html#numerical" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We cover solutions to several potential issues when performing direct numerical maximization of the likelihood (using the <a href="introhmm.html#forsec">forward algorithm</a>)</p>
<ol style="list-style-type: decimal">
<li><p><a href="numerical.html#likscale">Scaling the likelihood</a> to avoid numerical under/over-flow</p></li>
<li><p><a href="numerical.html#reparam">Reparameterizing</a> in order to use an unconstrained optimizer</p></li>
<li><p><a href="numerical.html#startval">Choosing a range of starting values</a> to avoid multiple maxima when finding the global maximum</p></li>
</ol>
<div id="likscale" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Scaling the Likelihood Computation<a href="numerical.html#likscale" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The forward probabilities of <span class="math inline">\(\boldsymbol{\alpha_t}\)</span> become progressively smaller as <span class="math inline">\(t\)</span> increases (see <a href="introhmm.html#introhmm">Exercise 1b</a>), which may lead to numerical underflow. Since the likelihood is a product of matrices, not of scalars, it is not possible to circumvent numerical underflow by simply computing the log of the likelihood. Instead, the vector of forward probabilities <span class="math inline">\(\boldsymbol{\alpha_t}\)</span> can be scaled at each time <span class="math inline">\(t\)</span>, so that the log likelihood is a sum of the logs of the scale factors.</p>
<p>Define, for <span class="math inline">\(t = 0, 1, \dots, T\)</span>, the vector</p>
<p><span class="math display">\[\boldsymbol{\phi_t} = \frac{\boldsymbol{\alpha_t}}{w_t}\]</span></p>
<p>where <span class="math inline">\(w_t = \sum_i \alpha_t(i) = \boldsymbol{\alpha_t 1&#39;}\)</span></p>
<p>By the definitions of <span class="math inline">\(\boldsymbol{\phi_t}\)</span> and <span class="math inline">\(w_t\)</span>, the immediate consequences are the following:</p>
<p><span class="math display">\[\begin{align}
&amp;\tag{1} \qquad &amp;w_0 &amp;= \boldsymbol{\alpha_0 1&#39;} = \boldsymbol{\delta 1&#39;} = 1\\
&amp;\tag{2} \qquad &amp;\boldsymbol{\phi_0} &amp;= \boldsymbol{\delta}\\
&amp;\tag{3} &amp; w_t \boldsymbol{\phi_t} &amp;= w_{t-1} \boldsymbol{\phi_{t-1} \Gamma P} (x_t)\\
&amp;\tag{4} &amp; L_T = \boldsymbol{\alpha_T 1&#39;} &amp;= w_T (\boldsymbol{\phi_T 1&#39;}) = w_T
\end{align}\]</span></p>
<p>Thus, the log-likelihood is</p>
<p><span class="math display" id="eq:scalelik">\[\begin{equation}\log L_T = \sum_{t=1}^T \log(\frac{w_t}{w_{t-1}}) = \sum_{t=1}^T \log(\boldsymbol{\phi_{t-1} \Gamma P} (x_t) \boldsymbol{1&#39;})
\tag{4.1}
\end{equation}\]</span></p>
<p><strong>Note:</strong></p>
<p><span class="math inline">\((1)\)</span> follows from <span class="math inline">\(w_0 = \boldsymbol{\alpha_0 1&#39;} = \boldsymbol{\delta 1&#39;} = \delta_1 + \cdots \delta_m = 1\)</span></p>
<p><span class="math inline">\((2)\)</span> follows from <span class="math inline">\(\boldsymbol{\phi_0} = \frac{\boldsymbol{\alpha_0}}{w_0} = \boldsymbol{\delta} \frac{1}{1} = \boldsymbol{\delta}\)</span></p>
<p><span class="math inline">\((3)\)</span> follows from <span class="math inline">\(\boldsymbol{\phi_t} = \frac{\boldsymbol{\alpha_t}}{w_t} \Rightarrow w_t \boldsymbol{\phi_t} = \boldsymbol{\alpha_t} = \boldsymbol{\alpha_{t-1} \Gamma P} (x_t) = w_{t-1} \boldsymbol{\phi_{t-1} \Gamma P} (x_t)\)</span></p>
<p><span class="math inline">\((4)\)</span> follows from <span class="math inline">\(L_T = \boldsymbol{\alpha_T 1&#39;} = w_T(\boldsymbol{\phi_T 1&#39;}) = w_T (\sum_T \phi_T) = w_T (\sum_T \frac{\alpha_T}{w_T} ) = w_T\)</span></p>
<p>Equation <a href="numerical.html#eq:scalelik">(4.1)</a> follows from <span class="math inline">\(L_T = w_T = \frac{w_1}{w_0} \frac{w_2}{w_1} \cdots \frac{w_T}{w_{T-1}} = \prod_{t=1}^T \frac{w_t}{w_{t-1}} = (\boldsymbol{\phi_{t-1} \Gamma P} (x_t) \boldsymbol{1&#39;}\)</span>, which follows from <span class="math inline">\((3)\)</span> <span class="math inline">\(w_t = w_{t-1} (\boldsymbol{\phi_{t-1} \Gamma P} (x_t) \boldsymbol{1&#39;})\)</span>.</p>
</div>
<div id="reparam" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Reparameterization to Avoid Constraints<a href="numerical.html#reparam" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The parameters of the Markov chain, state-dependent distributions, and initial distributions must be reparameterized in order to use an unconstrained optimizer, such as <code>nlm</code>.</p>
<div id="transition-probabilities" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Transition Probabilities<a href="numerical.html#transition-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the transition probabilities <span class="math inline">\(\gamma_{ij}\)</span>, to ensure that <span class="math inline">\(\sum_{j} \gamma_{ij} = 1\)</span> and <span class="math inline">\(\gamma_{ij} \geq 0\)</span>, apply the following reparameterization:</p>
<ol style="list-style-type: decimal">
<li><p>Define a new matrix <span class="math inline">\(\boldsymbol{T}\)</span> with entries <span class="math inline">\(\tau_{ij}\)</span> for <span class="math inline">\(i \neq j\)</span></p></li>
<li><p>Choose a strictly increasing function <span class="math inline">\(g: \mathbb{R} \rightarrow \mathbb{R}^+\)</span></p></li>
<li><p>Define new entries <span class="math inline">\(\nu_{ij}\)</span> where</p></li>
</ol>
<p><span class="math display">\[\begin{equation}
  \nu_{ij} = \left\{\begin{aligned}
  &amp; g(\tau_{ij}) &amp;&amp; \text{ for } i \neq j\\
  &amp;1 &amp;&amp; \text{ for } i = j\\
\end{aligned}
\right.
\end{equation}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Maximize <span class="math inline">\(L_T\)</span> with respect to <span class="math inline">\(\boldsymbol{T}\)</span> (and state-dependent parameters and initial probabilities)</p></li>
<li><p>Transform back</p></li>
</ol>
<p><span class="math display">\[\hat{\gamma_{ij}} = \frac{\hat{\nu}_{ij}}{\sum_{k=1}^m \hat{\nu}_{ik}} = \frac{g^{-1} (\hat{\tau}_{ik})}{\sum_{k=1, k \neq i}^m g^{-1} (\hat{\tau}_{ik})} \qquad{\text{ for }} i, j = 1, 2, \dots, m\]</span>.</p>
</div>
<div id="parameters-of-state-dependent-distributions" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Parameters of State-Dependent Distributions<a href="numerical.html#parameters-of-state-dependent-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For parameters of the state-dependent distribution,</p>
<ol style="list-style-type: decimal">
<li>Define new parameters</li>
</ol>
<p><span class="math inline">\(\qquad{\text{i.}}\)</span> If <span class="math inline">\(X_t \sim \text{Poisson} (\lambda_i)\)</span>, then define <span class="math inline">\(\eta_i = \log \lambda_i\)</span> for <span class="math inline">\(i = 1, \dots, m\)</span></p>
<p><span class="math inline">\(\qquad{\text{ii.}}\)</span> If <span class="math inline">\(X_t \sim \text{Binomial} (p_i)\)</span>, then define <span class="math inline">\(\eta_i = \log {\frac{p_i}{1 - p_i}}\)</span> for <span class="math inline">\(i = 1, \dots, m\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Maximize <span class="math inline">\(L_T\)</span> with respect to <span class="math inline">\(\eta_i\)</span> (and transition and initial probabilities)</p></li>
<li><p>Transform back</p></li>
</ol>
<p><span class="math inline">\(\qquad{\text{i. }} \hat{\boldsymbol{\lambda}} = \exp(\boldsymbol{\hat{\eta}})\)</span></p>
<p><span class="math inline">\(\qquad{\text{ii. }} \hat{\boldsymbol{p}} = \frac{\exp(\boldsymbol{\hat{\eta}})}{1 + \exp{\boldsymbol{\hat{\eta}}}}\)</span></p>
</div>
<div id="initial-probabilities" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Initial Probabilities<a href="numerical.html#initial-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the initial probabilities <span class="math inline">\(\delta_i\)</span>, to ensure that <span class="math inline">\(\delta_i \geq 0\)</span> and <span class="math inline">\(\sum_i \delta_i = 1\)</span>,</p>
<ol style="list-style-type: decimal">
<li>Define new parameters</li>
</ol>
<p><span class="math display">\[\pi_i = \log(\delta_i)\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Maximize <span class="math inline">\(L_T\)</span> with respect to <span class="math inline">\(\eta_i\)</span> (and state-dependent parameters and transition probabilities)</p></li>
<li><p>Transform back</p></li>
</ol>
<p><span class="math display">\[\hat{\boldsymbol{\delta}} = \exp(\hat{\boldsymbol{\pi}})\]</span></p>
</div>
</div>
<div id="startval" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Strategies for Choosing Starting Values<a href="numerical.html#startval" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There may be multiple maxima (i.e. local maxima) in the likelihood. There is no simple method of determining in general whether a numerical maximization has reached the global maximum. A strategy is to use a range of starting values for the maximization, and see whether the same maximum is identified in each case.</p>
<p>Possible starting values for the state-dependent distribution could be to take values close to measures of location.</p>
<ul>
<li><p>E.g. For a two-state Poisson-HMM, use values slightly smaller and slightly larger than the mean as starting values.</p></li>
<li><p>E.g. For a three-state Poisson-HMM, use the lower quartile, median, and upper quartile of the observed counts as starting values.</p></li>
</ul>
<p>Possible starting values for the transition probabilities could be to assign a common starting value to all the off-diagonal transition probabilities.</p>
<p>Possible starting values for the stationary distribution could be to assign uniform probability to all the states.</p>
</div>
<div id="boot" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Obtaining Standard Errors and Confidence Intervals<a href="numerical.html#boot" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Parametric bootstrapping</strong> uses fitted parameters <span class="math inline">\(\hat{\boldsymbol{\Theta}}\)</span> to generate <strong>bootstrap</strong> samples of observations, which can be used to evaluate properties of the model and obtain standard errors and confidence intervals.</p>
<p>To obtain the variance-covariance matrix of <span class="math inline">\(\boldsymbol{\Theta}\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Fit the model (i.e. compute <span class="math inline">\(\boldsymbol{\Theta}\)</span>)</p></li>
<li><ol style="list-style-type: lower-alpha">
<li>Generate a sample of observations from the fitted model of the same length as the original number of observations (i.e. generate bootstrap samples from the model with parameters <span class="math inline">\(\boldsymbol{\hat{\Theta}}\)</span>)</li>
</ol></li>
</ol>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Estimate the parameters <span class="math inline">\(\boldsymbol{\Theta}\)</span> by <span class="math inline">\(\boldsymbol{\hat{\Theta}^*}\)</span> for the bootstrap sample</p></li>
<li><p>Repeat steps <span class="math inline">\((a)\)</span> and <span class="math inline">\((b)\)</span> for (a large number of) <em>B</em> times and record the values <span class="math inline">\(\boldsymbol{\hat{\Theta}}^*\)</span></p></li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Estimate the variance-covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\Theta}}\)</span> by the sample variance-covariance matrix of the boostrap estimates <span class="math inline">\(\boldsymbol{\hat{\Theta}}^* (b)\)</span> for <span class="math inline">\(b= 1, 2, \dots, B\)</span></li>
</ol>
<p><span class="math display">\[\widehat{\text{Var-Cov}} (\hat{\boldsymbol{\Theta}}) = \frac{1}{B-1} \sum_{b=1}^B (\hat{\boldsymbol{\Theta}}^* (b) - \hat{\boldsymbol{\Theta}}^*(\cdot))&#39;(\hat{\boldsymbol{\Theta}}^* (b) - \hat{\boldsymbol{\Theta}}^*(\cdot))\]</span></p>
<p>where <span class="math inline">\(\hat{\boldsymbol{\Theta}}^*(\cdot) = B^{-1} \sum_{b=1}^B \hat{\boldsymbol{\Theta}}^*(b)\)</span></p>
<p>The bootstrap confidence intervals can be obtained by the ‘percentile method’ where the <span class="math inline">\(100 - \alpha \%\)</span> confidence interval has lower and upper limits of the <span class="math inline">\(\frac{\alpha}{2}\)</span>- and <span class="math inline">\(100 - \frac{\alpha}{2}-\)</span> th percentile.</p>
<p><strong>Note:</strong> See Section 3.6.1 of the textbook for a discussion of obtaining standard errors and confidence intervals using the Hessian.</p>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Exercises<a href="numerical.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Consider the following parameterization of the tpm of an <span class="math inline">\(m\)</span>-state Markov chain. Let <span class="math inline">\(\tau_{ij} \in \mathbb{R} (i, j = 1, 2, \dots, m,; i \neq j)\)</span> be <span class="math inline">\(m(m-1)\)</span> arbitrary real numbers. Let <span class="math inline">\(g: \mathbb{R} \rightarrow \mathbb{R}^+\)</span> be some strictly increasing function, e.g. <span class="math inline">\(g(x) = e^x\)</span>. Define <span class="math inline">\(\nu_{ij}\)</span> and <span class="math inline">\(\gamma_{ij}\)</span> as from <a href="numerical.html#reparam">the above</a></p>
<ol style="list-style-type: lower-alpha">
<li><p>Show that the matrix <span class="math inline">\(\boldsymbol{\Gamma}\)</span> with entries <span class="math inline">\(\gamma_{ij}\)</span> that are constructed in this way is a tpm.</p></li>
<li><p>Given <span class="math inline">\(m \times m\)</span> tpm <span class="math inline">\(\boldsymbol{\Gamma} = (\gamma_{ij})\)</span>, derive an expression for the parameters <span class="math inline">\(\tau_{ij}\)</span>, for <span class="math inline">\(i, j = 1, 2, \dots, m\)</span>; <span class="math inline">\(i \neq j\)</span>.</p></li>
</ol></li>
</ol>
</div>
<div id="solutions-2" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Solutions<a href="numerical.html#solutions-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Part a</strong>
We will first show that the rows of <span class="math inline">\(\boldsymbol{\Gamma}\)</span> sum to 1.</p>
<p>Let <span class="math inline">\(i = 1, \dots, m\)</span>. Then</p>
<p><span class="math display">\[\begin{align*}
\sum_{j=1}^m \gamma_{ij}
&amp;= \sum_{j=1}^m \frac{\nu_{ij}}{\sum_{k=1}^m \nu_{ik}}\\
&amp;= \frac{1}{1 + \sum_{k=1, k \neq i}^m g(\tau_{ik})} + \sum_{k=1, k \neq i}^m \frac{g (\tau_{ik})}{1 + \sum_{k=1, k \neq i}^m g(\tau_{ik})}\\
&amp;= \frac{1 + \sum_{k=1, k \neq i}^m g(\tau_{ik})}{1 + \sum_{k=1, k \neq i}^m g(\tau_{ik})}\\
&amp;= 1
\end{align*}\]</span></p>
<p>Thus <span class="math inline">\(\sum_{j=1}^m \gamma_{ij} = 1\)</span>.</p>
<p>Now we will show that the entries of <span class="math inline">\(\boldsymbol{\Gamma}\)</span> are between 0 and 1.</p>
<p>Since <span class="math inline">\(g: \mathbb{R} \rightarrow \mathbb{R}^+\)</span>, it follows from our definition of <span class="math inline">\(\gamma_{ij}\)</span> that</p>
<p><span class="math display">\[\begin{align}
  \gamma_{ij} &amp;= \frac{\nu_{ij}}{\sum_{k=1}^m \nu_{ik}} =
  \left\{\begin{aligned}
  &amp;\frac{1}{1 + \sum_{k=1, k \neq i}^m g(\tau_{ik})} \geq \frac{1}{1 + 0} = 1 &amp;&amp; \text{ if } i = j\\
  &amp; \frac{g (\tau_{ij})}{1 + \sum_{k=1, k \neq i}^m g(\tau_{ik})} \geq \frac{0}{1 + 0} = 0 &amp;&amp; \text{ if } i \neq j\\
\end{aligned}
\right.
\end{align}\]</span></p>
<p>Thus, <span class="math inline">\(\gamma_{ij} \geq 0\)</span>.</p>
<p>Then</p>
<p><span class="math inline">\(1 = \sum_{j=1}^m \gamma_{ij} \geq \gamma_{ij}\)</span> since <span class="math inline">\(\gamma_{ij}\)</span> are non-negative.</p>
<p>Thus, <span class="math inline">\(\gamma_{ij} \leq 1\)</span>.</p>
<p>Therefore, the constructed <span class="math inline">\(\boldsymbol{\Gamma}\)</span> is a tpm.</p>
<p><strong>Part b</strong></p>
<p>Let <span class="math inline">\(i, j = 1, 2, \dots, m\)</span>. Define <span class="math inline">\(g^{-1}\)</span> to be the inverse function of <span class="math inline">\(g\)</span>, <span class="math inline">\(g^{-1}: \mathbb{R}^+ \rightarrow \mathbb{R}\)</span>.</p>
<p>Then</p>
<p><span class="math display">\[\begin{align}
g^{-1} \left(\frac{\gamma_{ij}}{\gamma_{ii}}\right)
&amp;= g^{-1} \left(\gamma_{ij} (\frac{1}{\gamma_{ii}}) \right)\\
&amp;= g^{-1} \left(\gamma_{ij} (\frac{1 + \sum_{k=1, k \neq i}^m \nu_{ik}}{\nu_{ii}}) \right)
\qquad{\text{since } \gamma_{ii} = \frac{\nu_{ii}}{\sum_{k=1}^m \nu_{ik}}}\\
&amp;= g^{-1} \left(\gamma_{ij} (\frac{1 + \sum_{k=1, k \neq i}^m g(\tau_{ik})}{1}) \right)
\qquad{\text{by definition of } \nu_{ii}}\\
&amp;= g^{-1} \left(\gamma_{ij} (1 + \sum_{k=1, k \neq i}^m g (\tau_{ik})) \right)\\
&amp;= g^{-1} \left(g (\tau_{ij})\right)
\qquad{\text{since } \gamma_{ij} = \frac{\nu_{ij}}{\sum_{k=1}^m \nu_{ik}} = \frac{g(\tau_{ij})}{1 + \sum_{k = 1, k \neq i}^m \nu_{ik}}, i \neq j}\\
&amp;= \tau_{ij}
\end{align}\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introhmm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="fbalg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vinky-wang/HMM-Notes/edit/BRANCH/03-numerical-maximization.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "https://github.com/vinky-wang/HMM-Notes/raw/BRANCH/03-numerical-maximization.Rmd"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
